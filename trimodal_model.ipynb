{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ① Install NumPy from the regular PyPI repository (version 1.26.4 used here as an example)\n",
        "!pip install --no-cache-dir \"numpy<2.0\"\n",
        "\n",
        "# ② Next, install PyTorch / vision / audio from the cu121 index\n",
        "!pip install --no-cache-dir \\\n",
        "  torch==2.2.2+cu121 torchvision==0.17.2+cu121 torchaudio==2.2.2+cu121 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "kfPAm9zvk1YD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90Jqp9gb3ald",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')   # First, mount Google Drive normally\n",
        "\n",
        "!pip -q install pydub\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import warnings\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files  # <-- Required for downloading to local machine\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0) Path Settings (Existing folders are read-only)\n",
        "# ------------------------------------------------------------\n",
        "BASE_EXTR = \"/content/drive/MyDrive/DAIC-WOZ/extracted\"  # Existing\n",
        "PHQ_CSV   = \"/content/combined_PHQ_sorted.csv\"           # Existing\n",
        "\n",
        "TMP_ROOT  = \"/content/m5ov1_tmp\"                         # Local temporary directory\n",
        "TMP_AUD   = f\"{TMP_ROOT}/audio\"\n",
        "TMP_VIS   = f\"{TMP_ROOT}/visual\"\n",
        "os.makedirs(TMP_AUD, exist_ok=True)\n",
        "os.makedirs(TMP_VIS, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Parameters\n",
        "# ------------------------------------------------------------\n",
        "N_MERGE = 5      # Number of segments to merge\n",
        "OVL     = 1      # Number of overlapping segments\n",
        "STEP    = N_MERGE - OVL      # = 4\n",
        "FPS     = 30.0   # Frame rate from OpenFace\n",
        "SR      = 16000  # Sample rate for audio\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) TRANSCRIPT Loader\n",
        "# ------------------------------------------------------------\n",
        "def load_transcript(path):\n",
        "    rows = []\n",
        "    with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        next(f, None)  # Skip the header row\n",
        "        for ln in f:\n",
        "            pts = ln.strip().split(maxsplit=3)\n",
        "            if len(pts) < 4:\n",
        "                continue\n",
        "            s, e, spk, val = pts\n",
        "            rows.append(dict(start_time=float(s),\n",
        "                             stop_time=float(e),\n",
        "                             speaker=spk,\n",
        "                             value=val))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) AUDIO Segment Generation (to local temp directory)\n",
        "# ------------------------------------------------------------\n",
        "def extract_audio(pid:int):\n",
        "    tr  = f\"{BASE_EXTR}/{pid}_P/{pid}_TRANSCRIPT.csv\"\n",
        "    wav = f\"{BASE_EXTR}/{pid}_P/{pid}_AUDIO.wav\"\n",
        "    if not (os.path.exists(tr) and os.path.exists(wav)):\n",
        "        warnings.warn(f\"[AUDIO] missing {pid}\")\n",
        "        return []\n",
        "\n",
        "    df   = load_transcript(tr)\n",
        "    parts = df[df.speaker=='Participant']\n",
        "    full  = AudioSegment.from_wav(wav)\n",
        "\n",
        "    segs = [full[int(r.start_time*1000):int(r.stop_time*1000)] for _,r in parts.iterrows()]\n",
        "    out  = []\n",
        "    for i in range(0, len(segs)-N_MERGE+1, STEP):\n",
        "        merged = sum(segs[i:i+N_MERGE])\n",
        "        name   = f\"{pid}_m5ov1_{i//STEP}.wav\"\n",
        "        path   = f\"{TMP_AUD}/{name}\"\n",
        "        merged.export(path, format=\"wav\")\n",
        "        out.append(path)\n",
        "    return out\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) VISUAL Segment Generation (to local temp directory)\n",
        "# ------------------------------------------------------------\n",
        "def _safe_read(fp):\n",
        "    try:  # Try reading as comma-separated\n",
        "        return pd.read_csv(fp, sep=\",\", header=0)\n",
        "    except Exception: # Fallback to whitespace-delimited\n",
        "        return pd.read_csv(fp, delim_whitespace=True, header=0)\n",
        "\n",
        "def _rename_dups(df, existing_cols, tag):\n",
        "    new_cols = []\n",
        "    for c in df.columns:\n",
        "        if c in existing_cols and c != 'timestamp':\n",
        "            new_cols.append(f\"{c}{tag}\")\n",
        "        else:\n",
        "            new_cols.append(c)\n",
        "    df.columns = new_cols\n",
        "    return df\n",
        "\n",
        "def load_clnf(pid:int):\n",
        "    folder = f\"{BASE_EXTR}/{pid}_P\"\n",
        "    files  = [\n",
        "        f\"{pid}_CLNF_pose.txt\",\n",
        "        f\"{pid}_CLNF_gaze.txt\",\n",
        "        f\"{pid}_CLNF_features3D.txt\",\n",
        "        f\"{pid}_CLNF_features.txt\",\n",
        "        f\"{pid}_CLNF_AUs.txt\"\n",
        "    ]\n",
        "    merged = None\n",
        "    for fn in files:\n",
        "        fp = f\"{folder}/{fn}\"\n",
        "        if not os.path.exists(fp):\n",
        "            continue\n",
        "        df = _safe_read(fp)\n",
        "        df.columns = [c.strip() for c in df.columns]\n",
        "        if \"timestamp\" not in df.columns and \"frame\" in df.columns:\n",
        "            df[\"timestamp\"] = df[\"frame\"] / FPS\n",
        "        if merged is None:\n",
        "            merged = df\n",
        "        else:\n",
        "            tag = f\"_{fn.split('_')[-1].split('.')[0]}\"\n",
        "            df = _rename_dups(df, set(merged.columns), tag)\n",
        "            merged = merged.merge(df, on=\"timestamp\", how=\"outer\")\n",
        "    return pd.DataFrame() if merged is None else merged.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "def extract_visual(pid:int):\n",
        "    tr = f\"{BASE_EXTR}/{pid}_P/{pid}_TRANSCRIPT.csv\"\n",
        "    if not os.path.exists(tr):\n",
        "        return []\n",
        "    df_t = load_transcript(tr)\n",
        "    parts = df_t[df_t.speaker=='Participant']\n",
        "    df_c = load_clnf(pid)\n",
        "    if df_c.empty:\n",
        "        return []\n",
        "\n",
        "    segs = []\n",
        "    for _,r in parts.iterrows():\n",
        "        seg = df_c[(df_c.timestamp >= r.start_time) & (df_c.timestamp <= r.stop_time)]\n",
        "        segs.append(seg)\n",
        "\n",
        "    out = []\n",
        "    for i in range(0, len(segs)-N_MERGE+1, STEP):\n",
        "        merged = pd.concat(segs[i:i+N_MERGE], ignore_index=True)\n",
        "        name   = f\"{pid}_m5ov1_{i//STEP}.csv\"\n",
        "        path   = f\"{TMP_VIS}/{name}\"\n",
        "        merged.to_csv(path, index=False)\n",
        "        out.append(path)\n",
        "    return out\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Load PHQ Labels\n",
        "# ------------------------------------------------------------\n",
        "df_phq = pd.read_csv(PHQ_CSV)\n",
        "label_map = dict(zip(df_phq.Participant_ID, df_phq.PHQ8_Binary))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6) Generation Loop (writing to local disk)\n",
        "# ------------------------------------------------------------\n",
        "audio_all, vis_all, labels = [], [], []\n",
        "for pid, lbl in label_map.items():\n",
        "    a_list = extract_audio(pid)\n",
        "    v_list = extract_visual(pid)\n",
        "    ln = min(len(a_list), len(v_list))\n",
        "    for i in range(ln):\n",
        "        audio_all.append(a_list[i])\n",
        "        vis_all.append(v_list[i])\n",
        "        labels.append(lbl)\n",
        "\n",
        "# Save metadata CSV to the local temp directory\n",
        "meta_csv = f\"{TMP_ROOT}/dataset_info_all_ov1.csv\"\n",
        "pd.DataFrame(dict(audio_path=audio_all,\n",
        "                  visual_path=vis_all,\n",
        "                  label=labels)).to_csv(meta_csv, index=False)\n",
        "\n",
        "print(f\"✅ Local generation done: {len(audio_all)} segments\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7) Zip -> Copy to Drive -> Download to Local\n",
        "# ------------------------------------------------------------\n",
        "ts       = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "zip_path = f\"/content/m5ov1_segments_{ts}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "    for root, _, files_in_dir in os.walk(TMP_ROOT):\n",
        "        for fn in files_in_dir:\n",
        "            abs_path = os.path.join(root, fn)\n",
        "            rel_path = os.path.relpath(abs_path, TMP_ROOT)\n",
        "            zf.write(abs_path, arcname=rel_path)\n",
        "\n",
        "print(f\"🎁 Zipped → {zip_path}\")\n",
        "\n",
        "drive_dest = f\"/content/drive/MyDrive/DAIC-WOZ/m5ov1_segments_{ts}.zip\"\n",
        "shutil.copy(zip_path, drive_dest)\n",
        "print(f\"🚀 Copied to Drive → {drive_dest}\")\n",
        "\n",
        "# Download directly in Colab (be mindful of the file size)\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# One-shot script to merge o1pro prediction\n",
        "# into the 5-merge / 1-overlap dataset.\n",
        "# ============================================================\n",
        "!pip -q install pandas\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, zipfile, re\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- 0) Set paths ----------\n",
        "ZIP_PATH   = \"/content/drive/MyDrive/DAIC-WOZ/m5ov1_segments_20250422_152451.zip\"  # <- Path to the saved ZIP file\n",
        "TEXT_PATH  = \"/content/text-modality-result.csv\"    # <- Path to the text prediction results (ID, o1pro-prediction)\n",
        "WORK_DIR   = \"/content/m5ov1_data\"                  # Local extraction destination\n",
        "OUT_CSV    = \"/content/drive/MyDrive/DAIC-WOZ/dataset_info_all_text_ov1.csv\"\n",
        "\n",
        "# ---------- 1) Unzip the archive (only once) ----------\n",
        "if not os.path.exists(f\"{WORK_DIR}/dataset_info_all_ov1.csv\"):\n",
        "    os.makedirs(WORK_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "        zf.extractall(WORK_DIR)\n",
        "    print(f\"✔️  Unzipped into {WORK_DIR}\")\n",
        "\n",
        "# ---------- 2) Load the base CSV ----------\n",
        "base_csv = f\"{WORK_DIR}/dataset_info_all_ov1.csv\"\n",
        "df_base  = pd.read_csv(base_csv, low_memory=False)\n",
        "\n",
        "# If 'participant_id' column doesn't exist, extract it from the audio filename\n",
        "if \"participant_id\" not in df_base.columns:\n",
        "    def get_pid(path):\n",
        "        fn = os.path.basename(path)\n",
        "        m  = re.match(r\"(\\d+)_\", fn)\n",
        "        return int(m.group(1)) if m else -1\n",
        "    df_base[\"participant_id\"] = df_base[\"audio_path\"].apply(get_pid)\n",
        "\n",
        "# ---------- 3) Load the text results ----------\n",
        "df_text = pd.read_csv(TEXT_PATH)\n",
        "df_text = df_text.rename(columns={\"ID\": \"participant_id\"})        # Align column names for merging\n",
        "\n",
        "# ---------- 4) Merge (left join) ----------\n",
        "df_merged = pd.merge(\n",
        "    df_base,\n",
        "    df_text,                # <- This brings in the 'o1pro-prediction' column\n",
        "    on=\"participant_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Fill NaN (no prediction) with \"not depressed\"\n",
        "df_merged[\"o1pro-prediction\"] = df_merged[\"o1pro-prediction\"].fillna(\"not depressed\")\n",
        "\n",
        "# ---------- 5) Save the result ----------\n",
        "df_merged.to_csv(OUT_CSV, index=False)\n",
        "print(f\"✅ Saved {len(df_merged)} rows → {OUT_CSV}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez4lY-v73oJc",
        "outputId": "71665507-4f7e-4c3a-9393-6346454adda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✔️  Unzipped into /content/m5ov1_data\n",
            "✅ Saved 7972 rows → /content/drive/MyDrive/DAIC-WOZ/dataset_info_all_text_ov1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# multimodal_daicwoz_v3_attnpool_dropout.py\n",
        "# ------------------------------------------------------------\n",
        "# Tri‑modal depression detector (audio + visual + text)\n",
        "# This version introduces two changes that recent papers have\n",
        "# proven effective for unbalanced multimodal settings where one\n",
        "# modality dominates (DeepMLF, 2025; ECA‑MMDD, 2024; AVTF‑TBN, 2024):\n",
        "#   1) AttentivePooling instead of plain average‑pool ⇒ lets the\n",
        "#      network focus on depressive salient frames (turn‑level).\n",
        "#   2) ModalityDropout ⇒ randomly masks each modality w.p.=p at\n",
        "#      training time so that the network cannot over‑rely on text\n",
        "#      alone and must learn complementary audio‑visual cues.\n",
        "# The rest of the pipeline, hyper‑parameters, and CLI behaviour stay\n",
        "# identical to v2 so you can drop‑in replace the script.\n",
        "# ============================================================\n",
        "\n",
        "\"\"\"\n",
        "Prerequisites (same as v2):\n",
        "  !pip install torch torchaudio transformers librosa scikit-learn pydub\n",
        "  Upload DAIC‑WOZ pre‑processed files +\n",
        "  dataset_info_all_text.csv(audio_path, visual_path, participant_id,\n",
        "                            label, o1pro-prediction)\n",
        "  to Google Drive.\n",
        "Run:\n",
        "  python multimodal_daicwoz_v3_attnpool_dropout.py\n",
        "\"\"\"\n",
        "\n",
        "import os, gc, random, warnings\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2Model,\n",
        "    Wav2Vec2Config,\n",
        ")\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ------------------------------\n",
        "# 0. Common utilities\n",
        "# ------------------------------\n",
        "SEED = 103\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Dataset & collate_fn\n",
        "# ------------------------------\n",
        "class AudioVisualTextDataset(Dataset):\n",
        "    \"\"\"Loads audio, OpenFace CSV, and text label.\"\"\"\n",
        "\n",
        "    TEXT_MAP = {\"depressed\": 1.0, \"not depressed\": 0.0}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        target_sr: int = 16000,\n",
        "        expected_vis_dim: int = 393,\n",
        "        verbose: bool = False,\n",
        "    ):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.target_sr = target_sr\n",
        "        self.expected_vis_dim = expected_vis_dim\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        pid = row[\"participant_id\"]\n",
        "        label = int(row[\"label\"])\n",
        "        audio_path = row[\"audio_path\"]\n",
        "        visual_path = row[\"visual_path\"]\n",
        "        text_str = str(row.get(\"o1pro-prediction\", \"not depressed\")).strip().lower()\n",
        "        text_label = float(self.TEXT_MAP.get(text_str, 0.0))\n",
        "\n",
        "        try:\n",
        "            # --- audio ---\n",
        "            wav, sr = torchaudio.load(audio_path)\n",
        "            if wav.size(0) > 1:\n",
        "                wav = wav[:1]  # mono\n",
        "            if sr != self.target_sr:\n",
        "                wav = torchaudio.functional.resample(wav, sr, self.target_sr)\n",
        "                sr = self.target_sr\n",
        "\n",
        "            # --- visual ---\n",
        "            df_v = pd.read_csv(visual_path)\n",
        "            if \"timestamp\" in df_v.columns:\n",
        "                df_v.drop(columns=[\"timestamp\"], inplace=True)\n",
        "            df_v = df_v.select_dtypes(include=[np.number])\n",
        "            if df_v.shape[1] != self.expected_vis_dim:\n",
        "                raise ValueError(\"visual dim mismatch\")\n",
        "            vis = torch.tensor(df_v.values, dtype=torch.float32)\n",
        "\n",
        "            return idx, pid, wav, sr, vis, text_label, label\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                warnings.warn(f\"skip {pid}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [x for x in batch if x is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "\n",
        "    idxs, pids, waves, srs, vis_list, txt_labels, labels = zip(*batch)\n",
        "\n",
        "    # --- audio ---\n",
        "    wave_1d = [w.squeeze(0) for w in waves]\n",
        "    wave_pad = pad_sequence(wave_1d, batch_first=True)  # (B, T)\n",
        "    wave_pad = wave_pad.unsqueeze(1)  # (B, 1, T)\n",
        "    sr_tensor = torch.tensor(srs)\n",
        "\n",
        "    # --- visual ---\n",
        "    vis_pad = pad_sequence(vis_list, batch_first=True)  # (B, T_v, D)\n",
        "    vis_len = torch.tensor([v.size(0) for v in vis_list])\n",
        "\n",
        "    # --- text label ---\n",
        "    txt_tensor = torch.tensor(txt_labels, dtype=torch.float32)  # (B,)\n",
        "\n",
        "    labels_t = torch.tensor(labels, dtype=torch.long)\n",
        "    return pids, wave_pad, sr_tensor, vis_pad, vis_len, txt_tensor, labels_t\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Optional Focal Loss\n",
        "# ------------------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma: float = 2.0, weight: torch.Tensor | None = None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, weight=self.weight, reduction=\"none\")\n",
        "        pt = torch.exp(-ce)\n",
        "        focal = ((1 - pt) ** self.gamma) * ce\n",
        "        return focal.mean()\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Model blocks: AttentivePool & ModalityDropout\n",
        "# ------------------------------\n",
        "class AttentivePool(nn.Module):\n",
        "    \"\"\"Attention‑weighted mean pooling over the time dimension.\"\"\"\n",
        "\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.query = nn.Parameter(torch.randn(dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor | None = None):\n",
        "        # x: (B, T, D)\n",
        "        scores = (x * self.query).sum(-1)  # (B, T)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask, -1e9)\n",
        "        weights = torch.softmax(scores, dim=-1).unsqueeze(-1)  # (B, T, 1)\n",
        "        return (weights * x).sum(1)  # (B, D)\n",
        "\n",
        "class ModalityDropout(nn.Module):\n",
        "    \"\"\"Randomly zeros out the given modality embedding with prob p.\"\"\"\n",
        "\n",
        "    def __init__(self, p: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        if not self.training:\n",
        "            return x\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            return torch.zeros_like(x)\n",
        "        return x\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Fusion model\n",
        "# ------------------------------\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\", return_attention_mask=False)\n",
        "\n",
        "def wave_to_input(wave_batch, sr_batch, target_sr=16000):\n",
        "    out_list = []\n",
        "    for i in range(wave_batch.size(0)):\n",
        "        w = wave_batch[i, 0].cpu().numpy()\n",
        "        sr_i = sr_batch[i].item()\n",
        "        if sr_i != target_sr:\n",
        "            w = torchaudio.functional.resample(torch.from_numpy(w), sr_i, target_sr).numpy()\n",
        "        out_list.append(processor(w, sampling_rate=target_sr, return_tensors=\"pt\").input_values[0])\n",
        "    return pad_sequence(out_list, batch_first=True)\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim: int, heads: int = 8, p: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.av = nn.MultiheadAttention(dim, heads, dropout=p, batch_first=True)\n",
        "        self.va = nn.MultiheadAttention(dim, heads, dropout=p, batch_first=True)\n",
        "        self.n_a = nn.LayerNorm(dim)\n",
        "        self.n_v = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, A, V, mask_a=None, mask_v=None):\n",
        "        v2a, _ = self.av(V, A, A, key_padding_mask=mask_a)\n",
        "        V = self.n_v(V + v2a)\n",
        "        a2v, _ = self.va(A, V, V, key_padding_mask=mask_v)\n",
        "        A = self.n_a(A + a2v)\n",
        "        return A, V\n",
        "\n",
        "class GatingUnit(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(2, dim))\n",
        "\n",
        "    def forward(self, a_vec, v_vec):\n",
        "        weights = F.softmax(self.alpha.mean(dim=1), dim=0)\n",
        "        return weights[0] * a_vec + weights[1] * v_vec\n",
        "\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        audio_model_name=\"facebook/wav2vec2-base\",\n",
        "        unfreeze_last_n: int = 2,\n",
        "        visual_dim: int = 393,\n",
        "        hidden: int = 384,\n",
        "        heads: int = 8,\n",
        "        text_emb_dim: int = 128,\n",
        "        drop_p: float = 0.3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # ---- audio encoder ----\n",
        "        cfg = Wav2Vec2Config.from_pretrained(audio_model_name)\n",
        "        self.wav = Wav2Vec2Model.from_pretrained(audio_model_name)\n",
        "        for p in self.wav.parameters():\n",
        "            p.requires_grad = False\n",
        "        if unfreeze_last_n > 0:\n",
        "            for p in self.wav.encoder.layers[-unfreeze_last_n:].parameters():\n",
        "                p.requires_grad = True\n",
        "        self.proj_a = nn.Linear(cfg.hidden_size, hidden)\n",
        "\n",
        "        # ---- visual encoder ----\n",
        "        self.lstm = nn.LSTM(visual_dim, hidden // 2, num_layers=2,\n",
        "                             bidirectional=True, batch_first=True)\n",
        "        self.proj_v = nn.Identity()\n",
        "\n",
        "        # ---- cross attention ----\n",
        "        self.cross = CrossAttentionBlock(hidden, heads)\n",
        "        self.pool_a = AttentivePool(hidden)\n",
        "        self.pool_v = AttentivePool(hidden)\n",
        "\n",
        "        # ---- gating ----\n",
        "        self.gate = GatingUnit(hidden)\n",
        "\n",
        "        # ---- text embedding ----\n",
        "        self.text_emb = nn.Sequential(\n",
        "            nn.Linear(1, text_emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(text_emb_dim, text_emb_dim),\n",
        "        )\n",
        "        self.text_md = ModalityDropout(drop_p)\n",
        "\n",
        "        # ---- classifier ----\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.LayerNorm(hidden + text_emb_dim),\n",
        "            nn.Linear(hidden + text_emb_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, wav_inputs, vis, vis_mask, text_labels):\n",
        "        # ---- audio ----\n",
        "        A = self.wav(wav_inputs).last_hidden_state  # (B, T_a, H0)\n",
        "        A = self.proj_a(A)\n",
        "\n",
        "        # ---- visual ----\n",
        "        V, _ = self.lstm(vis)\n",
        "        V = self.proj_v(V)\n",
        "\n",
        "        # ---- cross attention ----\n",
        "        A, V = self.cross(A, V, mask_a=None, mask_v=vis_mask)\n",
        "        A_vec = self.pool_a(A)\n",
        "        V_vec = self.pool_v(V, mask=vis_mask)\n",
        "        av_fused = self.gate(A_vec, V_vec)  # (B, hidden)\n",
        "\n",
        "        # ---- text embedding (with ModalityDropout) ----\n",
        "        t_emb = self.text_emb(text_labels.unsqueeze(-1))  # (B, text_emb_dim)\n",
        "        t_emb = self.text_md(t_emb)\n",
        "\n",
        "        fused = torch.cat([av_fused, t_emb], dim=-1)\n",
        "        return self.cls(fused)\n",
        "\n",
        "# ------------------------------\n",
        "# 5. Helper functions\n",
        "# ------------------------------\n",
        "\n",
        "def make_mask(lengths: torch.Tensor, max_len: int) -> torch.Tensor:\n",
        "    idx = torch.arange(max_len, device=lengths.device).expand(len(lengths), -1)\n",
        "    return idx >= lengths.unsqueeze(1)\n",
        "\n",
        "# ------------------------------\n",
        "# 6. Training / Evaluation\n",
        "# ------------------------------\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    tot_loss, tot_samples, correct = 0.0, 0, 0\n",
        "\n",
        "    y_true_seg, y_pred_seg, y_text_seg, pid_list = [], [], [], []\n",
        "    part_logits, part_cnt, part_label, part_text_sum = {}, {}, {}, {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "            pids, wav, sr, vis, vlen, txt, y = batch\n",
        "            wav, sr = wav.to(device), sr.to(device)\n",
        "            vis, vlen = vis.to(device), vlen.to(device)\n",
        "            txt, y = txt.to(device), y.to(device)\n",
        "            txt_int = torch.round(txt).long()\n",
        "\n",
        "            wav_in = wave_to_input(wav, sr).to(device)\n",
        "            mask_v = make_mask(vlen, vis.size(1))\n",
        "            logits = model(wav_in, vis, mask_v, txt)\n",
        "\n",
        "            loss = criterion(logits, y)\n",
        "            bs = y.size(0)\n",
        "            tot_loss += loss.item() * bs\n",
        "            tot_samples += bs\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "\n",
        "            y_true_seg.extend(y.cpu().tolist())\n",
        "            y_pred_seg.extend(preds.cpu().tolist())\n",
        "            y_text_seg.extend(txt_int.cpu().tolist())\n",
        "            pid_list.extend(pids)\n",
        "\n",
        "            for i in range(bs):\n",
        "                pid = pids[i]\n",
        "                if pid not in part_logits:\n",
        "                    part_logits[pid] = logits[i].cpu()\n",
        "                    part_cnt[pid] = 1\n",
        "                    part_label[pid] = y[i].item()\n",
        "                    part_text_sum[pid] = txt_int[i].item()\n",
        "                else:\n",
        "                    part_logits[pid] += logits[i].cpu()\n",
        "                    part_cnt[pid] += 1\n",
        "                    part_text_sum[pid] += txt_int[i].item()\n",
        "\n",
        "    if tot_samples == 0:\n",
        "        return (0, 0, \"\", [[0,0],[0,0]], 0, \"\", [[0,0],[0,0]], \"\", [[0,0],[0,0]], \"\", [[0,0],[0,0]])\n",
        "\n",
        "    avg_loss_seg = tot_loss / tot_samples\n",
        "    acc_seg = correct / tot_samples\n",
        "    rpt_seg = classification_report(y_true_seg, y_pred_seg, digits=4,\n",
        "                                    target_names=[\"not dep\",\"dep\"])\n",
        "    cm_seg  = confusion_matrix(y_true_seg, y_pred_seg).tolist()\n",
        "\n",
        "    rpt_text_seg = classification_report(y_text_seg, y_pred_seg, digits=4,\n",
        "                                         target_names=[\"text=0\",\"text=1\"])\n",
        "    cm_text_seg  = confusion_matrix(y_text_seg, y_pred_seg).tolist()\n",
        "\n",
        "    y_true_part, y_pred_part, y_text_part = [], [], []\n",
        "    for pid, logit_sum in part_logits.items():\n",
        "        avg_log = logit_sum / part_cnt[pid]\n",
        "        y_pred_part.append(torch.argmax(avg_log).item())\n",
        "        y_true_part.append(part_label[pid])\n",
        "        txt_avg = part_text_sum[pid] / part_cnt[pid]\n",
        "        y_text_part.append(int(round(txt_avg)))\n",
        "\n",
        "    acc_part = (np.array(y_true_part) == np.array(y_pred_part)).mean()\n",
        "    rpt_part = classification_report(y_true_part, y_pred_part, digits=4,\n",
        "                                     target_names=[\"not dep\",\"dep\"])\n",
        "    cm_part = confusion_matrix(y_true_part, y_pred_part).tolist()\n",
        "\n",
        "    rpt_text_part = classification_report(y_text_part, y_pred_part, digits=4,\n",
        "                                          target_names=[\"text=0\",\"text=1\"])\n",
        "    cm_text_part  = confusion_matrix(y_text_part, y_pred_part).tolist()\n",
        "\n",
        "    return (\n",
        "        avg_loss_seg, acc_seg, rpt_seg, cm_seg,\n",
        "        acc_part, rpt_part, cm_part,\n",
        "        rpt_text_seg, cm_text_seg,\n",
        "        rpt_text_part, cm_text_part,\n",
        "    )\n",
        "\n",
        "# ------------------------------\n",
        "# 7. Main\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    CSV_PATH = \"/content/drive/MyDrive/DAIC-WOZ/dataset_info_all_text_ov1.csv\"\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "    df[\"audio_path\"]  = df[\"audio_path\"].str.replace(\n",
        "        \"/content/m5ov1_tmp\", \"/content/m5ov1_data\", regex=False)\n",
        "    df[\"visual_path\"] = df[\"visual_path\"].str.replace(\n",
        "        \"/content/m5ov1_tmp\", \"/content/m5ov1_data\", regex=False)\n",
        "\n",
        "    VIS_DIM = 393\n",
        "\n",
        "    ids = df[\"participant_id\"].unique()\n",
        "    train_ids, test_ids = train_test_split(ids, test_size=0.2, random_state=SEED)\n",
        "    train_ids, dev_ids  = train_test_split(train_ids, test_size=0.25, random_state=SEED)\n",
        "\n",
        "    train_df = df[df.participant_id.isin(train_ids)].copy()\n",
        "    dev_df   = df[df.participant_id.isin(dev_ids)].copy()\n",
        "    test_df  = df[df.participant_id.isin(test_ids)].copy()\n",
        "\n",
        "    BS, EPOCHS, ACC_STEPS, LR = 2, 15, 16, 1e-5\n",
        "    weight = torch.tensor([1.0, 2.5]); use_focal = False\n",
        "\n",
        "    train_ds = AudioVisualTextDataset(train_df, expected_vis_dim=VIS_DIM)\n",
        "    dev_ds   = AudioVisualTextDataset(dev_df,   expected_vis_dim=VIS_DIM)\n",
        "    test_ds  = AudioVisualTextDataset(test_df,  expected_vis_dim=VIS_DIM)\n",
        "\n",
        "    train_ld = DataLoader(train_ds, BS, shuffle=True,  collate_fn=collate_fn)\n",
        "    dev_ld   = DataLoader(dev_ds,  BS, shuffle=False, collate_fn=collate_fn)\n",
        "    test_ld  = DataLoader(test_ds, BS, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model  = MultiModalModel(unfreeze_last_n=2, visual_dim=VIS_DIM).to(device)\n",
        "\n",
        "    criterion = (FocalLoss(weight=weight.to(device)) if use_focal\n",
        "                 else nn.CrossEntropyLoss(weight=weight.to(device)))\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "    step = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train(); loss_sum = n = 0.0; opt.zero_grad()\n",
        "\n",
        "        for batch in train_ld:\n",
        "            if batch is None: continue\n",
        "            pids, wav, sr, vis, vlen, txt, y = batch\n",
        "            wav, sr = wav.to(device), sr.to(device)\n",
        "            vis, vlen = vis.to(device), vlen.to(device)\n",
        "            txt, y = txt.to(device), y.to(device)\n",
        "\n",
        "            x = wave_to_input(wav, sr).to(device)\n",
        "            out = model(x, vis, make_mask(vlen, vis.size(1)), txt)\n",
        "            loss = criterion(out, y) / ACC_STEPS\n",
        "            loss.backward()\n",
        "\n",
        "            loss_sum += loss.item() * y.size(0); n += y.size(0)\n",
        "            if (step + 1) % ACC_STEPS == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "                opt.step(); opt.zero_grad()\n",
        "            step += 1\n",
        "\n",
        "        print(f\"[E{epoch+1}] train_loss = {loss_sum / n if n else 0:.4f} (samples={n})\")\n",
        "\n",
        "        for name, loader in zip([\"dev\", \"test\"], [dev_ld, test_ld]):\n",
        "            (l_seg, acc_seg, rpt_seg, cm_seg,\n",
        "             acc_part, rpt_part, cm_part,\n",
        "             rpt_text_seg, cm_text_seg,\n",
        "             rpt_text_part, cm_text_part) = evaluate(model, loader, criterion, device)\n",
        "\n",
        "            print(f\"  === {name} (segment-level) ===\")\n",
        "            print(f\"    loss={l_seg:.4f}, acc={acc_seg:.3f}\")\n",
        "            print(\"    GroundTruth vs Pred:\");  print(rpt_seg)\n",
        "            print(f\"    CM:\\n      {cm_seg}\")\n",
        "            print(\"    TextLabel vs Pred:\");   print(rpt_text_seg)\n",
        "            print(f\"    CM:\\n      {cm_text_seg}\")\n",
        "\n",
        "            print(f\"  === {name} (participant-level) ===\")\n",
        "            print(f\"    acc={acc_part:.3f}\")\n",
        "            print(\"    GroundTruth vs Pred:\"); print(rpt_part)\n",
        "            print(f\"    CM:\\n      {cm_part}\")\n",
        "            print(\"    TextLabel vs Pred:\");   print(rpt_text_part)\n",
        "            print(f\"    CM:\\n      {cm_text_part}\")\n",
        "\n",
        "        gc.collect(); torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "V7DHxGXs31Cx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}