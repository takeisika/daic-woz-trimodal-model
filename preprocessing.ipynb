{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R74T4MUMbpq"
      },
      "source": [
        "# Download and Save DAIC-WOZ Files (zip) to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwwjggd8McWT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhdMF5tOMeHV"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/daic_tmp\n",
        "%cd /content/daic_tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTjZb0tHMffi"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "base_url=\"https://dcapswoz.ict.usc.edu/wwwdaicwoz/\"\n",
        "\n",
        "# Example: Download zip files for IDs 300 to 492\n",
        "for i in {300..492}; do\n",
        "  filename=\"${i}_P.zip\"\n",
        "  echo \"Downloading $filename from $base_url/$filename\"\n",
        "\n",
        "  # --continue: If the download is interrupted, attempt to resume\n",
        "  wget --continue \"$base_url/$filename\"\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4McTrrPMiE9"
      },
      "outputs": [],
      "source": [
        "# Create a folder in Drive for saving (if it doesn't already exist)\n",
        "!mkdir -p /content/drive/MyDrive/DAIC-WOZ\n",
        "\n",
        "# Copy files from the temporary directory to Drive\n",
        "!cp /content/daic_tmp/*.zip /content/drive/MyDrive/DAIC-WOZ/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3xQoWy2MkZF"
      },
      "source": [
        "# Unzip and Save DAIC-WOZ Files from Google Drive in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz09iPrHMmXI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Folder in Google Drive where the downloaded zip files are located\n",
        "# For example: /content/drive/MyDrive/DAIC-WOZ/zips\n",
        "ZIPS_DIR = '/content/drive/MyDrive/DAIC-WOZ'\n",
        "\n",
        "# Directory for extracting files (temporary folder)\n",
        "EXTRACT_DIR = '/content/daic_extracted'\n",
        "\n",
        "# Directory to save the cleaned transcripts after preprocessing\n",
        "CLEANED_DIR = '/content/drive/MyDrive/DAIC-WOZ/cleaned_transcripts'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ajiD5XDMoP_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "# Example for extracting zip files with IDs from 300 to 492\n",
        "for pid in range(300, 493):\n",
        "    zip_filename = f\"{pid}_P.zip\"\n",
        "    zip_path = os.path.join(ZIPS_DIR, zip_filename)\n",
        "\n",
        "    # Check if the zip file exists\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Not found: {zip_path}\")\n",
        "        continue\n",
        "\n",
        "    # Destination folder for extraction (e.g., /content/daic_extracted/300_P/)\n",
        "    out_dir = os.path.join(EXTRACT_DIR, f\"{pid}_P\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Extraction in the Colab environment\n",
        "    !unzip -q -o \"{zip_path}\" -d \"{out_dir}\"\n",
        "    print(f\"Extracted {zip_filename} to {out_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU23w5SwMqDx"
      },
      "outputs": [],
      "source": [
        "# Destination folder on Drive\n",
        "EXTRACT_DIR = '/content/drive/MyDrive/DAIC-WOZ/extracted'\n",
        "!mkdir -p \"{EXTRACT_DIR}\"\n",
        "\n",
        "# Copy from temporary folder to Drive\n",
        "!cp -r /content/daic_extracted/* \"{EXTRACT_DIR}/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBYRXOT6M08D"
      },
      "source": [
        "# Vocal Modality Prep (Extracting and Merging Patient Voice Segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2aHUdl0M7ln"
      },
      "outputs": [],
      "source": [
        "!pip install pydub\n",
        "import os\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "\n",
        "TRANSCRIPT_BASE_DIR = \"/content/drive/MyDrive/DAIC-WOZ/extracted\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/DAIC-WOZ/processed_patient_segments\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def load_transcript_custom(path):\n",
        "    \"\"\"\n",
        "    The first line is assumed to be the header ('start_time stop_time speaker value').\n",
        "    The subsequent lines are parsed manually:\n",
        "      - The first 3 tokens are start_time, stop_time, speaker\n",
        "      - Combine the 4th token onwards into value\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        # Skip the first line (header)\n",
        "        header_line = next(f, None)\n",
        "\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            # Split into up to 4 parts by whitespace\n",
        "            # parts[0] -> start_time\n",
        "            # parts[1] -> stop_time\n",
        "            # parts[2] -> speaker\n",
        "            # parts[3] -> value (full utterance)\n",
        "            parts = line.split(maxsplit=3)\n",
        "            if len(parts) < 4:\n",
        "                # If fewer than 4 parts are obtained, skip or adjust\n",
        "                continue\n",
        "\n",
        "            start_str, stop_str, spk, val = parts\n",
        "            # Convert to numeric values\n",
        "            start_time = float(start_str)\n",
        "            stop_time  = float(stop_str)\n",
        "\n",
        "            rows.append({\n",
        "                'start_time': start_time,\n",
        "                'stop_time':  stop_time,\n",
        "                'speaker':    spk,\n",
        "                'value':      val\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows,\n",
        "        columns=['start_time','stop_time','speaker','value']\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "def extract_patient_segments(audio_id, n_merge=5):\n",
        "    \"\"\"\n",
        "    1) Read /extracted/{audio_id}_P/{audio_id}_TRANSCRIPT.csv with the custom parser above.\n",
        "    2) Extract only the segments where speaker == \"Participant\".\n",
        "    3) Cut out segments from the actual .wav using pydub according to start_time and stop_time.\n",
        "    4) Concatenate every n_merge segments and write them to OUTPUT_DIR.\n",
        "    5) Return a list of output file paths.\n",
        "    \"\"\"\n",
        "    subfolder = f\"{audio_id}_P\"\n",
        "    transcript_file = f\"{audio_id}_TRANSCRIPT.csv\"\n",
        "    audio_file      = f\"{audio_id}_AUDIO.wav\"\n",
        "\n",
        "    transcript_path = os.path.join(TRANSCRIPT_BASE_DIR, subfolder, transcript_file)\n",
        "    audio_path      = os.path.join(TRANSCRIPT_BASE_DIR, subfolder, audio_file)\n",
        "\n",
        "    # Read the DataFrame using the custom parser\n",
        "    df = load_transcript_custom(transcript_path)\n",
        "\n",
        "    # Only Participant rows\n",
        "    df_patient = df[df['speaker'] == 'Participant'].copy()\n",
        "\n",
        "    # Load the entire audio using pydub\n",
        "    audio_full = AudioSegment.from_wav(audio_path)\n",
        "\n",
        "    segments = []\n",
        "    for _, row in df_patient.iterrows():\n",
        "        start_ms = row['start_time'] * 1000.0\n",
        "        end_ms   = row['stop_time']  * 1000.0\n",
        "        # Clip/cut out the segment\n",
        "        seg = audio_full[start_ms:end_ms]\n",
        "        segments.append(seg)\n",
        "\n",
        "    # Concatenate every n_merge segments\n",
        "    out_files = []\n",
        "    merged_count = 0\n",
        "    for i in range(0, len(segments), n_merge):\n",
        "        chunk_segs = segments[i:i+n_merge]\n",
        "        if not chunk_segs:\n",
        "            continue\n",
        "        merged = sum(chunk_segs)\n",
        "\n",
        "        out_filename = f\"{audio_id}_merged_{merged_count}.wav\"\n",
        "        out_path = os.path.join(OUTPUT_DIR, out_filename)\n",
        "        merged.export(out_path, format=\"wav\")\n",
        "        out_files.append(out_path)\n",
        "\n",
        "        merged_count += 1\n",
        "\n",
        "    return out_files\n",
        "\n",
        "# --- Example usage ---\n",
        "files_300 = extract_patient_segments(audio_id=300, n_merge=5)\n",
        "print(files_300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZispoj_M-62"
      },
      "source": [
        "# Visual Modality Prep (Merging CLNF Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TCva4uRNBZu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "TRANSCRIPT_BASE_DIR = \"/content/drive/MyDrive/DAIC-WOZ/extracted\"\n",
        "VISUAL_OUTPUT_DIR = \"/content/drive/MyDrive/DAIC-WOZ/processed_visual_segments\"\n",
        "os.makedirs(VISUAL_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def load_transcript_custom(path):\n",
        "    \"\"\"\n",
        "    The first line is assumed to be a header.\n",
        "    Read the subsequent lines, separating them into four parts:\n",
        "      - start_time, stop_time, speaker, value\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        # Skip the first line (header)\n",
        "        header_line = next(f, None)\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(maxsplit=3)\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "            start_str, stop_str, spk, val = parts\n",
        "            start_time = float(start_str)\n",
        "            stop_time  = float(stop_str)\n",
        "\n",
        "            rows.append({\n",
        "                'start_time': start_time,\n",
        "                'stop_time':  stop_time,\n",
        "                'speaker':    spk,\n",
        "                'value':      val\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=['start_time','stop_time','speaker','value'])\n",
        "    return df\n",
        "\n",
        "def load_clnf_data(audio_id: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Perform an outer join merge of CLNF files on the \"timestamp\" column.\n",
        "    - Do not read the hog file.\n",
        "    - If there are duplicate columns, use suffixes=(\"\", \"_dup\") to avoid collisions.\n",
        "    - Strip leading/trailing whitespace from column names.\n",
        "    \"\"\"\n",
        "    subfolder = f\"{audio_id}_P\"\n",
        "    folder_path = os.path.join(TRANSCRIPT_BASE_DIR, subfolder)\n",
        "\n",
        "    clnf_filenames = [\n",
        "        f\"{audio_id}_CLNF_pose.txt\",\n",
        "        f\"{audio_id}_CLNF_hog.txt\",  # skip hog\n",
        "        f\"{audio_id}_CLNF_gaze.txt\",\n",
        "        f\"{audio_id}_CLNF_features3D.txt\",\n",
        "        f\"{audio_id}_CLNF_features.txt\",\n",
        "        f\"{audio_id}_CLNF_AUs.txt\",\n",
        "    ]\n",
        "\n",
        "    df_merged = None\n",
        "    for fn in clnf_filenames:\n",
        "        fp = os.path.join(folder_path, fn)\n",
        "        if not os.path.exists(fp):\n",
        "            print(f\"[WARN] Not found: {fp}, skip.\")\n",
        "            continue\n",
        "\n",
        "        # Skip the hog file\n",
        "        if \"hog\" in fn.lower():\n",
        "            print(f\"[INFO] Skipping HOG file: {fn}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # First try reading using comma as delimiter\n",
        "            # (If it's actually whitespace-delimited, use delim_whitespace=True)\n",
        "            with open(fp, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                df_temp = pd.read_csv(f, sep=\",\", header=0)\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"[ERROR] Could not decode {fn} as UTF-8. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Strip leading/trailing whitespace from column names\n",
        "        df_temp.columns = [col.strip() for col in df_temp.columns]\n",
        "\n",
        "        # If timestamp column doesn't exist → create one from frames (assuming fps=30)\n",
        "        if \"timestamp\" not in df_temp.columns and \"frame\" in df_temp.columns:\n",
        "            df_temp[\"timestamp\"] = df_temp[\"frame\"] / 30.0\n",
        "\n",
        "        # Merge\n",
        "        if df_merged is None:\n",
        "            df_merged = df_temp\n",
        "            df_merged.columns = [col.strip() for col in df_merged.columns]\n",
        "        else:\n",
        "            df_merged.columns = [col.strip() for col in df_merged.columns]\n",
        "            df_merged = pd.merge(df_merged, df_temp,\n",
        "                                 on=\"timestamp\", how=\"outer\",\n",
        "                                 suffixes=(\"\", \"_dup\"))\n",
        "\n",
        "    if df_merged is None:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Sort by timestamp at the end\n",
        "    df_merged = df_merged.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    return df_merged\n",
        "\n",
        "def extract_visual_segments(audio_id, n_merge=5):\n",
        "    \"\"\"\n",
        "    1. Read the transcript.\n",
        "    2. Filter to only the rows with speaker == 'Participant'.\n",
        "    3. Load CLNF data.\n",
        "    4. For each Participant utterance, find data rows whose timestamps\n",
        "       lie between start_time and stop_time.\n",
        "    5. Concatenate every n_merge segments and output them as CSV.\n",
        "    6. Return the list of output file paths.\n",
        "    \"\"\"\n",
        "    subfolder = f\"{audio_id}_P\"\n",
        "    transcript_file = f\"{audio_id}_TRANSCRIPT.csv\"\n",
        "    transcript_path = os.path.join(TRANSCRIPT_BASE_DIR, subfolder, transcript_file)\n",
        "\n",
        "    df_trans = load_transcript_custom(transcript_path)\n",
        "    df_patient = df_trans[df_trans['speaker'] == 'Participant'].copy()\n",
        "\n",
        "    df_clnf = load_clnf_data(audio_id)\n",
        "    if df_clnf.empty:\n",
        "        print(f\"[WARN] No CLNF data loaded for {audio_id}\")\n",
        "        return []\n",
        "\n",
        "    segments = []\n",
        "    for _, row in df_patient.iterrows():\n",
        "        start_sec = row['start_time']\n",
        "        stop_sec  = row['stop_time']\n",
        "        mask = (df_clnf[\"timestamp\"] >= start_sec) & (df_clnf[\"timestamp\"] <= stop_sec)\n",
        "        seg_df = df_clnf[mask].copy()\n",
        "        segments.append(seg_df)\n",
        "\n",
        "    out_files = []\n",
        "    merged_count = 0\n",
        "    for i in range(0, len(segments), n_merge):\n",
        "        chunk = segments[i:i+n_merge]\n",
        "        if not chunk:\n",
        "            continue\n",
        "        merged_df = pd.concat(chunk, ignore_index=True)\n",
        "\n",
        "        out_filename = f\"{audio_id}_merged_{merged_count}.csv\"\n",
        "        out_path = os.path.join(VISUAL_OUTPUT_DIR, out_filename)\n",
        "        merged_df.to_csv(out_path, index=False)\n",
        "        out_files.append(out_path)\n",
        "\n",
        "        merged_count += 1\n",
        "\n",
        "    return out_files\n",
        "\n",
        "# === Example run ===\n",
        "files_300_visual = extract_visual_segments(audio_id=300, n_merge=5)\n",
        "print(\"Visual segments for 300:\", files_300_visual)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Am0Nj7NEOo"
      },
      "source": [
        "# Split and Save Segmented Audio/Visual Files to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX6fmjUHNHAC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "from pydub import AudioSegment\n",
        "\n",
        "#############################\n",
        "# 0) Settings and path specification\n",
        "#############################\n",
        "TRANSCRIPT_BASE_DIR = \"/content/drive/MyDrive/DAIC-WOZ/extracted\"\n",
        "\n",
        "OUTPUT_AUDIO_DIR = \"/content/drive/MyDrive/DAIC-WOZ/processed_patient_segments\"\n",
        "os.makedirs(OUTPUT_AUDIO_DIR, exist_ok=True)\n",
        "\n",
        "OUTPUT_VISUAL_DIR = \"/content/drive/MyDrive/DAIC-WOZ/processed_visual_segments\"\n",
        "os.makedirs(OUTPUT_VISUAL_DIR, exist_ok=True)\n",
        "\n",
        "# Path to combined_PHQ_sorted.csv\n",
        "PHQ_CSV_PATH = \"/content/combined_PHQ_sorted.csv\"\n",
        "\n",
        "\n",
        "#############################\n",
        "# 1) Function to read TRANSCRIPT\n",
        "#############################\n",
        "def load_transcript_custom(path):\n",
        "    \"\"\"\n",
        "    Reads DAIC-WOZ's {ID}_TRANSCRIPT.csv.\n",
        "    The first line is assumed to be a header (e.g. 'start_time stop_time speaker value').\n",
        "    From the second line onwards:\n",
        "      - The first three tokens: start_time, stop_time, speaker\n",
        "      - From the 4th token onwards: the utterance value\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    with open(path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        # Skip the header line\n",
        "        _ = next(f, None)\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(maxsplit=3)\n",
        "            if len(parts) < 4:\n",
        "                continue\n",
        "\n",
        "            start_str, stop_str, spk, val = parts\n",
        "            start_time = float(start_str)\n",
        "            stop_time  = float(stop_str)\n",
        "\n",
        "            rows.append({\n",
        "                'start_time': start_time,\n",
        "                'stop_time':  stop_time,\n",
        "                'speaker':    spk,\n",
        "                'value':      val\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=['start_time','stop_time','speaker','value'])\n",
        "    return df\n",
        "\n",
        "\n",
        "#############################\n",
        "# 2) Extract participant's audio segments\n",
        "#############################\n",
        "def extract_patient_segments(audio_id, n_merge=5):\n",
        "    \"\"\"\n",
        "    1) Read {audio_id}_TRANSCRIPT.csv and get rows where speaker=='Participant'\n",
        "    2) Use pydub to cut out the .wav file\n",
        "    3) Concatenate every n_merge segments -> write the result to OUTPUT_AUDIO_DIR\n",
        "    4) Return a list of output file paths\n",
        "    \"\"\"\n",
        "    subfolder = f\"{audio_id}_P\"\n",
        "    transcript_file = f\"{audio_id}_TRANSCRIPT.csv\"\n",
        "    audio_file      = f\"{audio_id}_AUDIO.wav\"\n",
        "    transcript_path = os.path.join(TRANSCRIPT_BASE_DIR, subfolder, transcript_file)\n",
        "    audio_path      = os.path.join(TRANSCRIPT_BASE_DIR, subfolder, audio_file)\n",
        "\n",
        "    df = load_transcript_custom(transcript_path)\n",
        "    df_patient = df[df['speaker'] == 'Participant'].copy()\n",
        "\n",
        "    audio_full = AudioSegment.from_wav(audio_path)\n",
        "\n",
        "    segments = []\n",
        "    for _, row in df_patient.iterrows():\n",
        "        start_ms = int(row['start_time'] * 1000)\n",
        "        end_ms   = int(row['stop_time']  * 1000)\n",
        "        seg = audio_full[start_ms:end_ms]\n",
        "        segments.append(seg)\n",
        "\n",
        "    out_files = []\n",
        "    merged_count = 0\n",
        "    for i in range(0, len(segments), n_merge):\n",
        "        chunk_segs = segments[i:i+n_merge]\n",
        "        if not chunk_segs:\n",
        "            continue\n",
        "        merged = sum(chunk_segs)\n",
        "        out_filename = f\"{audio_id}_merged_{merged_count}.wav\"\n",
        "        out_path = os.path.join(OUTPUT_AUDIO_DIR, out_filename)\n",
        "        merged.export(out_path, format=\"wav\")\n",
        "        out_files.append(out_path)\n",
        "        merged_count += 1\n",
        "\n",
        "    return out_files\n",
        "\n",
        "\n",
        "#############################\n",
        "# 3) Load CLNF (visual) files\n",
        "#############################\n",
        "def load_clnf_data(audio_id: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merge the CLNF files in extracted/{audio_id}_P/ by timestamp.\n",
        "    Skip the HOG file.\n",
        "    * If the files are actually whitespace-delimited, set delim_whitespace=True.\n",
        "    \"\"\"\n",
        "    subfolder = f\"{audio_id}_P\"\n",
        "    folder_path = os.path.join(TRANSCRIPT_BASE_DIR, subfolder)\n",
        "\n",
        "    clnf_filenames = [\n",
        "        f\"{audio_id}_CLNF_pose.txt\",\n",
        "        f\"{audio_id}_CLNF_hog.txt\",  # skip hog\n",
        "        f\"{audio_id}_CLNF_gaze.txt\",\n",
        "        f\"{audio_id}_CLNF_features3D.txt\",\n",
        "        f\"{audio_id}_CLNF_features.txt\",\n",
        "        f\"{audio_id}_CLNF_AUs.txt\",\n",
        "    ]\n",
        "\n",
        "    df_merged = None\n",
        "    for fn in clnf_filenames:\n",
        "        fp = os.path.join(folder_path, fn)\n",
        "        if not os.path.exists(fp):\n",
        "            print(f\"[WARN] Not found: {fp}, skip.\")\n",
        "            continue\n",
        "\n",
        "        if \"hog\" in fn.lower():\n",
        "            print(f\"[INFO] Skipping HOG file: {fn}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(fp, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                df_temp = pd.read_csv(f, sep=\",\", header=0)\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"[ERROR] Could not decode {fn}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Strip whitespace from column names\n",
        "        df_temp.columns = [c.strip() for c in df_temp.columns]\n",
        "\n",
        "        # If timestamp is missing, create it from frame (assuming fps=30)\n",
        "        if \"timestamp\" not in df_temp.columns and \"frame\" in df_temp.columns:\n",
        "            df_temp[\"timestamp\"] = df_temp[\"frame\"] / 30.0\n",
        "\n",
        "        if df_merged is None:\n",
        "            df_merged = df_temp\n",
        "        else:\n",
        "            df_merged.columns = [c.strip() for c in df_merged.columns]\n",
        "            df_merged = pd.merge(\n",
        "                df_merged, df_temp, on=\"timestamp\", how=\"outer\",\n",
        "                suffixes=(\"\", \"_dup\")\n",
        "            )\n",
        "\n",
        "    if df_merged is None:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_merged = df_merged.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "def extract_visual_segments(audio_id, n_merge=5):\n",
        "    \"\"\"\n",
        "    1) Retrieve Participant segments via load_transcript_custom\n",
        "    2) From the merged DataFrame in load_clnf_data, cut out rows whose timestamps lie within [start_time, stop_time]\n",
        "    3) Concatenate every n_merge segments -> output CSV -> return a list of paths\n",
        "    \"\"\"\n",
        "    subfolder = f\"{audio_id}_P\"\n",
        "    transcript_file = f\"{audio_id}_TRANSCRIPT.csv\"\n",
        "    transcript_path = os.path.join(TRANSCRIPT_BASE_DIR, subfolder, transcript_file)\n",
        "\n",
        "    df_trans = load_transcript_custom(transcript_path)\n",
        "    df_patient = df_trans[df_trans['speaker'] == 'Participant'].copy()\n",
        "\n",
        "    df_clnf = load_clnf_data(audio_id)\n",
        "    if df_clnf.empty:\n",
        "        print(f\"[WARN] No CLNF data for {audio_id}\")\n",
        "        return []\n",
        "\n",
        "    segments = []\n",
        "    for _, row in df_patient.iterrows():\n",
        "        start_sec = row['start_time']\n",
        "        stop_sec  = row['stop_time']\n",
        "        mask = (df_clnf[\"timestamp\"] >= start_sec) & (df_clnf[\"timestamp\"] <= stop_sec)\n",
        "        seg_df = df_clnf[mask].copy()\n",
        "        segments.append(seg_df)\n",
        "\n",
        "    out_files = []\n",
        "    merged_count = 0\n",
        "    for i in range(0, len(segments), n_merge):\n",
        "        chunk = segments[i:i+n_merge]\n",
        "        if not chunk:\n",
        "            continue\n",
        "        merged_df = pd.concat(chunk, ignore_index=True)\n",
        "        out_filename = f\"{audio_id}_merged_{merged_count}.csv\"\n",
        "        out_path = os.path.join(OUTPUT_VISUAL_DIR, out_filename)\n",
        "        merged_df.to_csv(out_path, index=False)\n",
        "        out_files.append(out_path)\n",
        "        merged_count += 1\n",
        "\n",
        "    return out_files\n",
        "\n",
        "\n",
        "#############################\n",
        "# 4) Collect segment extractions for all participants\n",
        "#############################\n",
        "def gather_all_segments(id_list, n_merge=5, label_dict=None):\n",
        "    \"\"\"\n",
        "    For each ID in id_list:\n",
        "      - Extract audio segments with extract_patient_segments\n",
        "      - Extract visual segments with extract_visual_segments\n",
        "      - Pair them up by the same index and assign label_dict[ID] as the label\n",
        "    \"\"\"\n",
        "    audio_paths_all = []\n",
        "    visual_paths_all = []\n",
        "    labels_all = []\n",
        "\n",
        "    for pid in id_list:\n",
        "        audio_list = extract_patient_segments(pid, n_merge=n_merge)\n",
        "        visual_list = extract_visual_segments(pid, n_merge=n_merge)\n",
        "\n",
        "        # Label (for example, PHQ8_Binary)\n",
        "        lbl = 0.0\n",
        "        if label_dict and pid in label_dict:\n",
        "            lbl = label_dict[pid]\n",
        "\n",
        "        min_len = min(len(audio_list), len(visual_list))\n",
        "        for i in range(min_len):\n",
        "            audio_paths_all.append(audio_list[i])\n",
        "            visual_paths_all.append(visual_list[i])\n",
        "            labels_all.append(lbl)\n",
        "\n",
        "    return audio_paths_all, visual_paths_all, labels_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnG_5X78NI7O"
      },
      "source": [
        "# Generate a Multimodal (Audio/Visual/Text) Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcLSJiJQNNZz"
      },
      "source": [
        "<h3>Collect and Match Audio/Visual Segments by Participant ID, Then Save to CSV</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6siCHOawNP13"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 1) Retrieve ID -> label (PHQ8_Binary) from combined_PHQ_sorted.csv\n",
        "# ----------------------------------------------------------------\n",
        "df_phq = pd.read_csv(PHQ_CSV_PATH, sep=\",\")  # Adjust the delimiter as appropriate for your data\n",
        "# If it's tab-delimited, change to sep=\"\\t\"\n",
        "\n",
        "# Assuming there are \"Participant_ID\" and \"PHQ8_Binary\" columns\n",
        "# Convert labels to a dictionary, e.g., label_dict[300] = 0, label_dict[301] = 1, ...\n",
        "label_dict = dict(zip(df_phq[\"Participant_ID\"], df_phq[\"PHQ8_Binary\"]))\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 2) From processed_* folders, match {ID}_merged_*.wav with {ID}_merged_*.csv\n",
        "# ----------------------------------------------------------------\n",
        "audio_paths_all = []\n",
        "visual_paths_all = []\n",
        "labels_all = []\n",
        "\n",
        "# Iterate over the ID -> label dictionary (includes IDs from 300 to 492 and possibly others)\n",
        "for pid, label in label_dict.items():\n",
        "    # Look for files containing pid in their names\n",
        "    # Example: \"300_merged_0.wav\", \"300_merged_1.wav\", etc.\n",
        "    wav_files = [fn for fn in os.listdir(AUDIO_DIR)\n",
        "                 if fn.startswith(f\"{pid}_merged_\") and fn.endswith(\".wav\")]\n",
        "\n",
        "    csv_files = [fn for fn in os.listdir(VISUAL_DIR)\n",
        "                 if fn.startswith(f\"{pid}_merged_\") and fn.endswith(\".csv\")]\n",
        "\n",
        "    # Function to extract the merged index (e.g., 0, 1, 12) from the filename\n",
        "    def get_merged_index(filename):\n",
        "        # Example filename: \"300_merged_12.wav\" or \"300_merged_12.csv\"\n",
        "        base, _ = os.path.splitext(filename)  # => \"300_merged_12\"\n",
        "        # Split after \"300_merged_\"\n",
        "        parts = base.split(\"_merged_\")\n",
        "        if len(parts) == 2:\n",
        "            # parts[0] = \"300\", parts[1] = \"12\"\n",
        "            return int(parts[1])  # convert \"12\" to 12\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Sort the files by the index at the end of the filename\n",
        "    wav_files_sorted = sorted(wav_files, key=get_merged_index)\n",
        "    csv_files_sorted = sorted(csv_files, key=get_merged_index)\n",
        "\n",
        "    # Match up audio and visual files for the minimum number of segments in common\n",
        "    # (e.g., if there are 5 audio segments and 7 visual segments, only match 5)\n",
        "    min_len = min(len(wav_files_sorted), len(csv_files_sorted))\n",
        "\n",
        "    for i in range(min_len):\n",
        "        # Audio file path\n",
        "        audio_path = os.path.join(AUDIO_DIR, wav_files_sorted[i])\n",
        "        # Visual CSV file path\n",
        "        visual_path = os.path.join(VISUAL_DIR, csv_files_sorted[i])\n",
        "\n",
        "        audio_paths_all.append(audio_path)\n",
        "        visual_paths_all.append(visual_path)\n",
        "        labels_all.append(label)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 3) Save the final matches to CSV\n",
        "# ----------------------------------------------------------------\n",
        "df_out = pd.DataFrame({\n",
        "    \"audio_path\": audio_paths_all,\n",
        "    \"visual_path\": visual_paths_all,\n",
        "    \"label\": labels_all\n",
        "})\n",
        "\n",
        "df_out.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"[INFO] Saved {len(df_out)} rows to {OUTPUT_CSV}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7gFf2GZNSAr"
      },
      "source": [
        "<h3>Merge Text Modality Predictions with Audio/Visual Dataset by Participant ID</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx6KcDzSNUQN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# ★ 1) Read dataset_info_all.csv\n",
        "DATASET_INFO_CSV = \"/content/drive/MyDrive/DAIC-WOZ/dataset_info_all.csv\"\n",
        "df_main = pd.read_csv(DATASET_INFO_CSV)\n",
        "\n",
        "# The columns in df_main are assumed to be ['audio_path','visual_path','label'], for example:\n",
        "# /content/drive/MyDrive/DAIC-WOZ/processed_patient_segments/301_merged_0.wav\n",
        "\n",
        "# Define a function to extract the \"Participant ID\" from the audio_path\n",
        "def extract_id_from_path(path):\n",
        "    \"\"\"\n",
        "    Example:\n",
        "      path = \"/content/drive/.../processed_patient_segments/301_merged_0.wav\"\n",
        "      => participant_id = 301 (int)\n",
        "    \"\"\"\n",
        "    # Get only the filename\n",
        "    filename = os.path.basename(path)  # => \"301_merged_0.wav\"\n",
        "\n",
        "    # Use a regular expression to extract the ID\n",
        "    # Example pattern: [digits]+(?=_merged_)\n",
        "    match = re.match(r\"(\\d+)_merged_\\d+\", filename)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Add a \"participant_id\" column to df_main\n",
        "df_main[\"participant_id\"] = df_main[\"audio_path\"].apply(extract_id_from_path)\n",
        "\n",
        "# ★ 2) Read text-modality-result.csv\n",
        "TEXT_MODALITY_CSV = \"/content/text-modality-result.csv\"\n",
        "df_text = pd.read_csv(TEXT_MODALITY_CSV)\n",
        "\n",
        "# df_text is assumed to have columns -> ['ID','o1pro-prediction']\n",
        "# Rename the ID column to \"participant_id\" to facilitate merging\n",
        "df_text = df_text.rename(columns={\"ID\": \"participant_id\"})\n",
        "\n",
        "# ★ 3) Merge\n",
        "df_merged = pd.merge(\n",
        "    df_main,\n",
        "    df_text,\n",
        "    on=\"participant_id\",  # Merge on participant_id\n",
        "    how=\"left\"            # Left join => rows from df_main remain; if an ID is not found, the new columns are NaN\n",
        ")\n",
        "\n",
        "# Now df_merged has the \"o1pro-prediction\" column (e.g., \"depressed\" or \"not depressed\")\n",
        "\n",
        "# ★ 4) Save to CSV (audio_path, visual_path, label, participant_id, o1pro-prediction)\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/DAIC-WOZ/dataset_info_all_text.csv\"\n",
        "df_merged.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"[INFO] Saved: {OUTPUT_CSV}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}