{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd87znnnkQ9z"
      },
      "outputs": [],
      "source": [
        "!pip install pydub\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "!pip install torch torchaudio transformers librosa scikit-learn\n",
        "PHQ_CSV_PATH = \"/content/combined_PHQ_sorted.csv\"\n",
        "AUDIO_DIR    = \"/content/drive/MyDrive/DAIC-WOZ/processed_patient_segments\"\n",
        "VISUAL_DIR   = \"/content/drive/MyDrive/DAIC-WOZ/processed_visual_segments\"\n",
        "OUTPUT_CSV   = \"/content/drive/MyDrive/DAIC-WOZ/dataset_info_all.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Firstly, the participant interview segment dataset was randomly split for training and testing. This method allows data from the same participant to appear in both sets, causing data leakage and resulting in an oddly high accuracy of 99% as epochs progress."
      ],
      "metadata": {
        "id": "mlDz474UklRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "!pip install torch torchaudio transformers librosa scikit-learn\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import Wav2Vec2Config, Wav2Vec2Model, Wav2Vec2Processor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Set random seed for reproducibility\n",
        "# -----------------------------------------------------\n",
        "SEED = 103\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 1) AudioDataset (Returns participant_id as well)\n",
        "# -----------------------------------------------------\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, target_sr=16000, verbose=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.target_sr = target_sr\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        audio_path = row[\"audio_path\"]\n",
        "        label = float(row[\"label\"])  # 0 or 1\n",
        "        participant_id = row[\"participant_id\"]  # <-- added\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"[Dataset] idx={idx}, audio_path={audio_path}, label={label}, participant_id={participant_id}\")\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "            # If the number of channels is 2 or more, convert to mono (use only the first channel)\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = waveform[0, :].unsqueeze(0)\n",
        "\n",
        "            if sr != self.target_sr:\n",
        "                waveform = torchaudio.functional.resample(waveform, sr, self.target_sr)\n",
        "                sr = self.target_sr\n",
        "\n",
        "            return (idx, waveform, sr, label, participant_id)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Return None if loading fails\n",
        "            print(f\"[WARN] Skipped sample idx={idx}, audio='{audio_path}' due to error: {e}\")\n",
        "            return None\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 2) collate_fn: skip None\n",
        "# -----------------------------------------------------\n",
        "def collate_fn_audio(batch):\n",
        "    # Exclude None\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None  # Skip in subsequent layers\n",
        "\n",
        "    sample_indices = []\n",
        "    waveforms = []\n",
        "    srs = []\n",
        "    labels = []\n",
        "    participant_ids = []\n",
        "\n",
        "    for (idx, wf, sr, lb, pid) in batch:\n",
        "        sample_indices.append(idx)\n",
        "        waveforms.append(wf)\n",
        "        srs.append(torch.tensor(sr))\n",
        "        labels.append(torch.tensor(lb))\n",
        "        participant_ids.append(pid)\n",
        "\n",
        "    # waveforms: List of [1, T]\n",
        "    wave_1d_list = [item.squeeze(0) for item in waveforms]  # => [T]\n",
        "    padded_wav_2d = pad_sequence(wave_1d_list, batch_first=True, padding_value=0.0)  # => (B,T)\n",
        "    padded_wav_3d = padded_wav_2d.unsqueeze(1)  # => (B,1,T)\n",
        "\n",
        "    labels_tensor = torch.stack(labels, dim=0)\n",
        "    sr_tensor = torch.stack(srs, dim=0)\n",
        "\n",
        "    return sample_indices, padded_wav_3d, sr_tensor, labels_tensor, participant_ids\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 3) Model Definition (Wav2Vec2 + mean pooling + classifier)\n",
        "# -----------------------------------------------------\n",
        "class Wav2Vec2AudioEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_name=\"facebook/wav2vec2-large-960h\",\n",
        "                 output_hidden_states=False,\n",
        "                 freeze_feature_extractor=True):\n",
        "        super().__init__()\n",
        "        self.config = Wav2Vec2Config.from_pretrained(model_name)\n",
        "        self.model = Wav2Vec2Model.from_pretrained(model_name)\n",
        "        if freeze_feature_extractor:\n",
        "            for param in self.model.feature_extractor.parameters():\n",
        "                param.requires_grad = False\n",
        "        self.model.config.output_hidden_states = output_hidden_states\n",
        "        self.hidden_size = self.config.hidden_size\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        outputs = self.model(input_values=input_values)\n",
        "        return outputs.last_hidden_state  # (B, T', hidden_size)\n",
        "\n",
        "class AudioOnlyModel(nn.Module):\n",
        "    \"\"\"\n",
        "    1) Feature extraction with Wav2Vec2\n",
        "    2) Linear transformation + mean pooling\n",
        "    3) Classification\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        audio_model_name=\"facebook/wav2vec2-large-960h\",\n",
        "        freeze_feature_extractor=True,\n",
        "        cross_hidden_dim=384,\n",
        "        num_classes=2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Audio encoder\n",
        "        self.audio_encoder = Wav2Vec2AudioEncoder(\n",
        "            model_name=audio_model_name,\n",
        "            freeze_feature_extractor=freeze_feature_extractor\n",
        "        )\n",
        "        audio_out_dim = self.audio_encoder.hidden_size  # 1024 for wav2vec2-large-960h\n",
        "\n",
        "        # Project to cross_hidden_dim\n",
        "        self.proj_audio = nn.Linear(audio_out_dim, cross_hidden_dim)\n",
        "        self.norm_audio = nn.LayerNorm(cross_hidden_dim)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(cross_hidden_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        # Audio\n",
        "        audio_embed = self.audio_encoder(input_values)  # (B, T_a, hidden_size)\n",
        "\n",
        "        # Project dimensions\n",
        "        A = self.proj_audio(audio_embed)  # (B, T_a, cross_hidden_dim)\n",
        "\n",
        "        # mean pooling\n",
        "        A_pool = A.mean(dim=1)  # (B, cross_hidden_dim)\n",
        "        A_pool = self.norm_audio(A_pool)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(A_pool)\n",
        "        return logits\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 4) wave_to_wav2vec2_input\n",
        "# -----------------------------------------------------\n",
        "processor = Wav2Vec2Processor.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-960h\",\n",
        "    return_attention_mask=False\n",
        ")\n",
        "\n",
        "def wave_to_wav2vec2_input(wave_batch, sr_batch, target_sr=16000):\n",
        "    \"\"\"\n",
        "    wave_batch : (B,1,T)\n",
        "    sr_batch : (B,)\n",
        "    \"\"\"\n",
        "    B = wave_batch.size(0)\n",
        "    input_values_list = []\n",
        "    for i in range(B):\n",
        "        wave_i = wave_batch[i, 0, :].cpu().numpy()\n",
        "        sr_i = sr_batch[i].item()\n",
        "        if sr_i != target_sr:\n",
        "            wave_i = torchaudio.functional.resample(\n",
        "                torch.from_numpy(wave_i), sr_i, target_sr\n",
        "            ).numpy()\n",
        "        out = processor(\n",
        "            wave_i,\n",
        "            sampling_rate=target_sr,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=False,\n",
        "            return_attention_mask=False\n",
        "        )\n",
        "        input_values_list.append(out.input_values[0])\n",
        "\n",
        "    # Pad if lengths are different\n",
        "    padded_input_values = pad_sequence(\n",
        "        input_values_list,\n",
        "        batch_first=True,\n",
        "        padding_value=0.0\n",
        "    )\n",
        "    return padded_input_values  # (B, T_wav2vec)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 5) Segment-level Evaluation\n",
        "# -----------------------------------------------------\n",
        "def evaluate_one_epoch(\n",
        "    model, data_loader, criterion, device, print_confusion=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate at the *segment level*.\n",
        "    Returns average loss, segment-level accuracy, recall of label=1,\n",
        "    and predictions/labels in lists (for optional further use).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            if batch is None:  # Skip if everything is None\n",
        "                continue\n",
        "\n",
        "            sample_indices, wave_batch, sr_batch, lbl_batch, pid_list = batch\n",
        "\n",
        "            wave_batch = wave_batch.to(device)\n",
        "            sr_batch = sr_batch.to(device)\n",
        "            lbl_batch = lbl_batch.to(device)\n",
        "\n",
        "            inputs = wave_to_wav2vec2_input(wave_batch, sr_batch, target_sr=16000).to(device)\n",
        "            logits = model(inputs)\n",
        "\n",
        "            loss_val = criterion(logits, lbl_batch.long())\n",
        "\n",
        "            bs = wave_batch.size(0)\n",
        "            total_loss += loss_val.item() * bs\n",
        "            total_samples += bs\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == lbl_batch.long()).sum().item()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(lbl_batch.cpu().numpy().tolist())\n",
        "\n",
        "    if total_samples > 0:\n",
        "        avg_loss = total_loss / total_samples\n",
        "        accuracy = correct / total_samples\n",
        "    else:\n",
        "        avg_loss = 0.0\n",
        "        accuracy = 0.0\n",
        "\n",
        "    # confusion matrix etc.\n",
        "    recall_1 = 0.0\n",
        "    if print_confusion and len(all_labels) > 0:\n",
        "        target_names = [\"not depressed\", \"depressed\"]\n",
        "        cr_str = classification_report(all_labels, all_preds, target_names=target_names, digits=6)\n",
        "        print(\"=== Classification Report (segment-level) ===\")\n",
        "        print(cr_str)\n",
        "\n",
        "        cr_dict = classification_report(all_labels, all_preds, target_names=target_names, digits=6, output_dict=True)\n",
        "        if \"depressed\" in cr_dict:\n",
        "            recall_1 = cr_dict[\"depressed\"][\"recall\"]\n",
        "\n",
        "        cm = confusion_matrix(all_labels, all_preds, labels=[0,1])\n",
        "        print(\"Confusion Matrix ( [0,1] ) (segment-level):\\n\", cm)\n",
        "\n",
        "    return avg_loss, accuracy, recall_1, all_preds, all_labels\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 6) Participant-level Evaluation\n",
        "# -----------------------------------------------------\n",
        "def evaluate_one_epoch_per_participant(\n",
        "    model, data_loader, criterion, device, print_confusion=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate at the *participant level* by aggregating all segment logits\n",
        "    belonging to the same participant.\n",
        "\n",
        "    Steps:\n",
        "      1) For each segment in the dataloader, compute logits (like normal).\n",
        "      2) Store [logits, label, participant_id].\n",
        "      3) After the loop, group by participant_id, average the logits,\n",
        "         derive final predictions, and compare to the label.\n",
        "\n",
        "    Note: This expects that *all segments from one participant* have the\n",
        "    same ground-truth label. The code uses the label from the first segment\n",
        "    we see for that participant, but you can adapt if your data differ.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_segments = 0\n",
        "    # We'll store the raw segment-level outputs here:\n",
        "    storage = []  # list of (participant_id, label, logits)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            sample_indices, wave_batch, sr_batch, lbl_batch, pid_list = batch\n",
        "\n",
        "            wave_batch = wave_batch.to(device)\n",
        "            sr_batch = sr_batch.to(device)\n",
        "            lbl_batch = lbl_batch.to(device)\n",
        "\n",
        "            inputs = wave_to_wav2vec2_input(wave_batch, sr_batch, target_sr=16000).to(device)\n",
        "            logits = model(inputs)  # (B, num_classes)\n",
        "\n",
        "            # Just accumulate the raw data\n",
        "            bs = wave_batch.size(0)\n",
        "            total_segments += bs\n",
        "            # We'll sum up the cross-entropy loss for info only\n",
        "            # (not exactly meaningful at participant-level directly)\n",
        "            loss_val = criterion(logits, lbl_batch.long())\n",
        "            total_loss += loss_val.item() * bs\n",
        "\n",
        "            for i in range(bs):\n",
        "                pid = pid_list[i]\n",
        "                label_i = lbl_batch[i].item()\n",
        "                logit_i = logits[i].cpu().numpy()\n",
        "                storage.append((pid, label_i, logit_i))\n",
        "\n",
        "    # Now do the participant-level aggregation\n",
        "    participant_dict = {}\n",
        "    for (pid, label_i, logit_i) in storage:\n",
        "        if pid not in participant_dict:\n",
        "            participant_dict[pid] = {\n",
        "                \"logits_list\": [],\n",
        "                \"label\": label_i\n",
        "            }\n",
        "        participant_dict[pid][\"logits_list\"].append(logit_i)\n",
        "\n",
        "    all_participant_preds = []\n",
        "    all_participant_labels = []\n",
        "\n",
        "    for pid, val in participant_dict.items():\n",
        "        label = val[\"label\"]\n",
        "        logits_list = val[\"logits_list\"]  # list of np arrays (num_classes,)\n",
        "\n",
        "        # Average\n",
        "        mean_logits = np.mean(logits_list, axis=0)  # shape (num_classes,)\n",
        "        pred_class = np.argmax(mean_logits)\n",
        "\n",
        "        all_participant_preds.append(pred_class)\n",
        "        all_participant_labels.append(label)\n",
        "\n",
        "    # Now compute participant-level metrics\n",
        "    n_participants = len(all_participant_labels)\n",
        "    if n_participants > 0:\n",
        "        avg_loss_seg = total_loss / total_segments  # segment-level average\n",
        "        # If we want \"average loss per participant,\" we could do\n",
        "        # total_loss / n_participants, but that doesn't always\n",
        "        # match your pipeline. It's up to you.\n",
        "        # For now, let's keep segment-level average for reference.\n",
        "\n",
        "        correct_p = sum(\n",
        "            1 for i in range(n_participants)\n",
        "            if all_participant_preds[i] == all_participant_labels[i]\n",
        "        )\n",
        "        participant_acc = correct_p / n_participants\n",
        "    else:\n",
        "        avg_loss_seg = 0.0\n",
        "        participant_acc = 0.0\n",
        "\n",
        "    recall_1 = 0.0\n",
        "    if print_confusion and len(all_participant_labels) > 0:\n",
        "        target_names = [\"not depressed\", \"depressed\"]\n",
        "        cr_str = classification_report(all_participant_labels, all_participant_preds,\n",
        "                                       target_names=target_names, digits=6)\n",
        "        print(\"=== Classification Report (participant-level) ===\")\n",
        "        print(cr_str)\n",
        "\n",
        "        cr_dict = classification_report(\n",
        "            all_participant_labels,\n",
        "            all_participant_preds,\n",
        "            target_names=target_names,\n",
        "            digits=6,\n",
        "            output_dict=True\n",
        "        )\n",
        "        if \"depressed\" in cr_dict:\n",
        "            recall_1 = cr_dict[\"depressed\"][\"recall\"]\n",
        "\n",
        "        cm = confusion_matrix(all_participant_labels, all_participant_preds, labels=[0,1])\n",
        "        print(\"Confusion Matrix ( [0,1] ) (participant-level):\\n\", cm)\n",
        "\n",
        "    return avg_loss_seg, participant_acc, recall_1\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 7) Main\n",
        "# -----------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # === Settings ===\n",
        "    CSV_PATH = \"/content/drive/MyDrive/DAIC-WOZ/dataset_info_all_text.csv\"\n",
        "    #\n",
        "    # IMPORTANT:\n",
        "    # Make sure your CSV has columns:\n",
        "    #  - \"audio_path\"\n",
        "    #  - \"label\" (0 or 1)\n",
        "    #  - \"participant_id\" (string or int).\n",
        "    #\n",
        "    # Example minimal CSV structure:\n",
        "    # participant_id, audio_path, label\n",
        "    # P001, /path/to/P001_seg1.wav, 0\n",
        "    # P001, /path/to/P001_seg2.wav, 0\n",
        "    # P002, /path/to/P002_seg1.wav, 1\n",
        "    # ... etc.\n",
        "\n",
        "    BATCH_SIZE = 2\n",
        "    accumulation_steps = 16\n",
        "\n",
        "    # Number of epochs\n",
        "    EPOCHS = 20\n",
        "\n",
        "    LEARNING_RATE = 1e-5\n",
        "\n",
        "    # Class weights [1.0, 2.5] (example)\n",
        "    class_weights = torch.FloatTensor([1.0, 2.5])\n",
        "\n",
        "    # Load CSV\n",
        "    df_all = pd.read_csv(CSV_PATH)\n",
        "    print(f\"Loaded CSV total rows: {len(df_all)}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # train/dev/test split (6:2:2)\n",
        "    # -------------------------------\n",
        "    df_train, df_temp = train_test_split(df_all, test_size=0.4, random_state=SEED, shuffle=True)\n",
        "    df_dev, df_test = train_test_split(df_temp, test_size=0.5, random_state=SEED, shuffle=True)\n",
        "    print(f\"Train: {len(df_train)}, Dev: {len(df_dev)}, Test: {len(df_test)}\")\n",
        "\n",
        "    # Create Dataset\n",
        "    train_dataset = AudioDataset(df_train, target_sr=16000, verbose=False)\n",
        "    dev_dataset   = AudioDataset(df_dev,   target_sr=16000, verbose=False)\n",
        "    test_dataset  = AudioDataset(df_test,  target_sr=16000, verbose=False)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_audio)\n",
        "    dev_loader   = DataLoader(dev_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_audio)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_audio)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"device={device}\")\n",
        "\n",
        "    # Model\n",
        "    model = AudioOnlyModel(\n",
        "        audio_model_name=\"facebook/wav2vec2-large-960h\",\n",
        "        freeze_feature_extractor=True,\n",
        "        cross_hidden_dim=384,\n",
        "        num_classes=2\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss_train = 0.0\n",
        "        total_samples_train = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # -------------------------\n",
        "        # Training loop (segment-level)\n",
        "        # -------------------------\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            sample_indices, wave_batch, sr_batch, lbl_batch, pid_list = batch\n",
        "\n",
        "            wave_batch = wave_batch.to(device)\n",
        "            sr_batch = sr_batch.to(device)\n",
        "            lbl_batch = lbl_batch.to(device)\n",
        "\n",
        "            inputs = wave_to_wav2vec2_input(wave_batch, sr_batch, 16000).to(device)\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, lbl_batch.long())\n",
        "\n",
        "            bs = wave_batch.size(0)\n",
        "            total_loss_train += loss.item() * bs\n",
        "            total_samples_train += bs\n",
        "\n",
        "            # Gradient accumulation\n",
        "            loss = loss / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (step + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "            # Simple log every 50 steps\n",
        "            if (global_step % 50) == 0:\n",
        "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "                print(f\"[Train] epoch={epoch+1}, global_step={global_step}, loss={loss.item():.4f}, lr={current_lr}\")\n",
        "\n",
        "        # Average training loss per epoch\n",
        "        if total_samples_train > 0:\n",
        "            epoch_train_loss = total_loss_train / total_samples_train\n",
        "        else:\n",
        "            epoch_train_loss = 0.0\n",
        "        print(f\"[Epoch {epoch+1}] train loss = {epoch_train_loss:.4f}\")\n",
        "\n",
        "        # -------------------------\n",
        "        # Dev Evaluation (segment-level)\n",
        "        # -------------------------\n",
        "        print(f\"=== [Epoch {epoch+1}] Dev Evaluation (Segment-Level) ===\")\n",
        "        dev_loss_eval, dev_acc_eval, dev_recall_1, _, _ = evaluate_one_epoch(\n",
        "            model, dev_loader, criterion, device, print_confusion=False\n",
        "        )\n",
        "        print(f\"Dev (segment-level): loss={dev_loss_eval:.4f}, acc={dev_acc_eval:.4f}, recall(depressed)={dev_recall_1:.4f}\")\n",
        "\n",
        "        # Dev Evaluation (participant-level)\n",
        "        print(f\"=== [Epoch {epoch+1}] Dev Evaluation (Participant-Level) ===\")\n",
        "        dev_loss_part, dev_acc_part, dev_recall_part = evaluate_one_epoch_per_participant(\n",
        "            model, dev_loader, criterion, device, print_confusion=False\n",
        "        )\n",
        "        print(f\"Dev (participant-level): [segment-based loss={dev_loss_part:.4f}], acc={dev_acc_part:.4f}, recall(depressed)={dev_recall_part:.4f}\")\n",
        "\n",
        "        # -------------------------\n",
        "        # Test Evaluation (segment-level)\n",
        "        # -------------------------\n",
        "        print(f\"=== [Epoch {epoch+1}] Test Evaluation (Segment-Level) ===\")\n",
        "        test_loss_eval, test_acc_eval, test_recall_1, _, _ = evaluate_one_epoch(\n",
        "            model, test_loader, criterion, device, print_confusion=True\n",
        "        )\n",
        "        print(f\"Test (segment-level): loss={test_loss_eval:.4f}, acc={test_acc_eval:.4f}, recall(depressed)={test_recall_1:.4f}\")\n",
        "\n",
        "        # Test Evaluation (participant-level)\n",
        "        print(f\"=== [Epoch {epoch+1}] Test Evaluation (Participant-Level) ===\")\n",
        "        test_loss_part, test_acc_part, test_recall_part = evaluate_one_epoch_per_participant(\n",
        "            model, test_loader, criterion, device, print_confusion=True\n",
        "        )\n",
        "        print(f\"Test (participant-level): [segment-based loss={test_loss_part:.4f}], acc={test_acc_part:.4f}, recall(depressed)={test_recall_part:.4f}\")\n",
        "\n",
        "        # Save the results (save any metrics as needed)\n",
        "        results.append({\n",
        "            'epoch': epoch+1,\n",
        "            'train_loss': epoch_train_loss,\n",
        "\n",
        "            'dev_loss_seg': dev_loss_eval,\n",
        "            'dev_acc_seg': dev_acc_eval,\n",
        "            'dev_recall_seg': dev_recall_1,\n",
        "\n",
        "            'dev_loss_part': dev_loss_part,  # note: this is segment-based average loss\n",
        "            'dev_acc_part': dev_acc_part,\n",
        "            'dev_recall_part': dev_recall_part,\n",
        "\n",
        "            'test_loss_seg': test_loss_eval,\n",
        "            'test_acc_seg': test_acc_eval,\n",
        "            'test_recall_seg': test_recall_1,\n",
        "\n",
        "            'test_loss_part': test_loss_part,\n",
        "            'test_acc_part': test_acc_part,\n",
        "            'test_recall_part': test_recall_part\n",
        "        })\n",
        "\n",
        "        # Output report every 5 epochs\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            df_report = pd.DataFrame(results)\n",
        "            save_path = f\"/content/drive/MyDrive/training_report_epoch_{epoch+1}.csv\"\n",
        "            df_report.to_csv(save_path, index=False)\n",
        "            print(f\"[Saved] {save_path}\")\n",
        "\n",
        "        print(f\"--- End of epoch {epoch+1} ---\\n\")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "    # Save the final report\n",
        "    df_report_final = pd.DataFrame(results)\n",
        "    save_path_final = f\"/content/drive/MyDrive/training_report_final.csv\"\n",
        "    df_report_final.to_csv(save_path_final, index=False)\n",
        "    print(f\"[Saved Final Report] {save_path_final}\")"
      ],
      "metadata": {
        "id": "_iN_2DYtkZ_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. However, separating participants for training and testing, with no other changes, causes the model's accuracy to drop significantly to 60%."
      ],
      "metadata": {
        "id": "vBv4kgDGkh2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "!pip install torch torchaudio transformers librosa scikit-learn\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import Wav2Vec2Config, Wav2Vec2Model, Wav2Vec2Processor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 0) Set seed for reproducibility\n",
        "# -----------------------------------------------------\n",
        "SEED = 103\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 1) AudioDataset\n",
        "#    (Return None and skip if reading audio fails)\n",
        "# -----------------------------------------------------\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, target_sr=16000, verbose=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.target_sr = target_sr\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        audio_path = row[\"audio_path\"]\n",
        "        label = float(row[\"label\"])  # 0 or 1\n",
        "        participant_id = row[\"participant_id\"]\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"[Dataset] idx={idx}, audio_path={audio_path}, label={label}, pid={participant_id}\")\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "            # If the number of channels is 2 or more, convert to mono (use only the first channel)\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = waveform[0, :].unsqueeze(0)\n",
        "\n",
        "            if sr != self.target_sr:\n",
        "                waveform = torchaudio.functional.resample(waveform, sr, self.target_sr)\n",
        "                sr = self.target_sr\n",
        "\n",
        "            return (idx, waveform, sr, label, participant_id)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Return None if loading fails\n",
        "            print(f\"[WARN] Skipped sample idx={idx}, audio='{audio_path}' due to error: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 2) collate_fn: skip None\n",
        "# -----------------------------------------------------\n",
        "def collate_fn_audio(batch):\n",
        "    # Filter out None values\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None  # Return None to be skipped in the training loop\n",
        "\n",
        "    sample_indices = []\n",
        "    waveforms = []\n",
        "    srs = []\n",
        "    labels = []\n",
        "    participant_ids = []\n",
        "\n",
        "    for (idx, wf, sr, lb, pid) in batch:\n",
        "        sample_indices.append(idx)\n",
        "        waveforms.append(wf)\n",
        "        srs.append(torch.tensor(sr))\n",
        "        labels.append(torch.tensor(lb))\n",
        "        participant_ids.append(pid)\n",
        "\n",
        "    # waveforms: List of tensors of shape [1, T]\n",
        "    wave_1d_list = [item.squeeze(0) for item in waveforms]  # => Convert to a list of 1D tensors [T]\n",
        "    padded_wav_2d = pad_sequence(wave_1d_list, batch_first=True, padding_value=0.0)  # => Pad to shape (B, T)\n",
        "    padded_wav_3d = padded_wav_2d.unsqueeze(1)  # => Add channel dimension to get (B, 1, T)\n",
        "\n",
        "    labels_tensor = torch.stack(labels, dim=0)\n",
        "    sr_tensor = torch.stack(srs, dim=0)\n",
        "\n",
        "    return sample_indices, padded_wav_3d, sr_tensor, labels_tensor, participant_ids\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 3) Model Definition (Wav2Vec2 + mean pooling + classifier)\n",
        "# -----------------------------------------------------\n",
        "class Wav2Vec2AudioEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_name=\"facebook/wav2vec2-large-960h\",\n",
        "                 output_hidden_states=False,\n",
        "                 freeze_feature_extractor=True):\n",
        "        super().__init__()\n",
        "        self.config = Wav2Vec2Config.from_pretrained(model_name)\n",
        "        self.model = Wav2Vec2Model.from_pretrained(model_name)\n",
        "        if freeze_feature_extractor:\n",
        "            for param in self.model.feature_extractor.parameters():\n",
        "                param.requires_grad = False\n",
        "        self.model.config.output_hidden_states = output_hidden_states\n",
        "        self.hidden_size = self.config.hidden_size\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        outputs = self.model(input_values=input_values)\n",
        "        return outputs.last_hidden_state  # (B, T', hidden_size)\n",
        "\n",
        "\n",
        "class AudioOnlyModel(nn.Module):\n",
        "    \"\"\"\n",
        "      1) Wav2Vec2 for feature extraction\n",
        "      2) Project to an intermediate dimension + mean pooling\n",
        "      3) Classification\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        audio_model_name=\"facebook/wav2vec2-large-960h\",\n",
        "        freeze_feature_extractor=True,\n",
        "        cross_hidden_dim=384,  # arbitrary\n",
        "        num_classes=2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Audio encoder\n",
        "        self.audio_encoder = Wav2Vec2AudioEncoder(\n",
        "            model_name=audio_model_name,\n",
        "            freeze_feature_extractor=freeze_feature_extractor\n",
        "        )\n",
        "        audio_out_dim = self.audio_encoder.hidden_size  # wav2vec2-large-960h => 1024\n",
        "\n",
        "        # Project to cross_hidden_dim\n",
        "        self.proj_audio = nn.Linear(audio_out_dim, cross_hidden_dim)\n",
        "        self.norm_audio = nn.LayerNorm(cross_hidden_dim)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(cross_hidden_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        # Audio\n",
        "        audio_embed = self.audio_encoder(input_values)  # (B, T_a, hidden_size)\n",
        "\n",
        "        # Linear projection\n",
        "        A = self.proj_audio(audio_embed)  # (B, T_a, cross_hidden_dim)\n",
        "\n",
        "        # Mean pooling\n",
        "        A_pool = A.mean(dim=1)  # (B, cross_hidden_dim)\n",
        "        A_pool = self.norm_audio(A_pool)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(A_pool)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 4) wave_to_wav2vec2_input\n",
        "# -----------------------------------------------------\n",
        "processor = Wav2Vec2Processor.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-960h\",\n",
        "    return_attention_mask=False\n",
        ")\n",
        "\n",
        "def wave_to_wav2vec2_input(wave_batch, sr_batch, target_sr=16000):\n",
        "    \"\"\"\n",
        "    wave_batch : (B, 1, T)\n",
        "    sr_batch   : (B,)\n",
        "    \"\"\"\n",
        "    B = wave_batch.size(0)\n",
        "    input_values_list = []\n",
        "    for i in range(B):\n",
        "        wave_i = wave_batch[i, 0, :].cpu().numpy()\n",
        "        sr_i = sr_batch[i].item()\n",
        "        if sr_i != target_sr:\n",
        "            wave_i = torchaudio.functional.resample(\n",
        "                torch.from_numpy(wave_i), sr_i, target_sr\n",
        "            ).numpy()\n",
        "        out = processor(\n",
        "            wave_i,\n",
        "            sampling_rate=target_sr,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=False,\n",
        "            return_attention_mask=False\n",
        "        )\n",
        "        input_values_list.append(out.input_values[0])\n",
        "\n",
        "    # Pad if lengths differ\n",
        "    padded_input_values = pad_sequence(\n",
        "        input_values_list,\n",
        "        batch_first=True,\n",
        "        padding_value=0.0\n",
        "    )\n",
        "    return padded_input_values  # (B, T_wav2vec)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 5-A) Segment-level Evaluation\n",
        "# -----------------------------------------------------\n",
        "def evaluate_one_epoch_segment_level(model, data_loader, criterion, device, print_confusion=True):\n",
        "    \"\"\"\n",
        "    Evaluate at the segment level (traditional approach).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            if batch is None:  # Skip if everything is None\n",
        "                continue\n",
        "\n",
        "            sample_indices, wave_batch, sr_batch, lbl_batch, participant_ids = batch\n",
        "\n",
        "            wave_batch = wave_batch.to(device)\n",
        "            sr_batch   = sr_batch.to(device)\n",
        "            lbl_batch  = lbl_batch.to(device)\n",
        "\n",
        "            inputs = wave_to_wav2vec2_input(wave_batch, sr_batch, target_sr=16000).to(device)\n",
        "            logits = model(inputs)\n",
        "\n",
        "            loss_val = criterion(logits, lbl_batch.long())\n",
        "\n",
        "            bs = wave_batch.size(0)\n",
        "            total_loss += loss_val.item() * bs\n",
        "            total_samples += bs\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == lbl_batch.long()).sum().item()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(lbl_batch.cpu().numpy().tolist())\n",
        "\n",
        "    if total_samples > 0:\n",
        "        avg_loss = total_loss / total_samples\n",
        "        accuracy = correct / total_samples\n",
        "    else:\n",
        "        avg_loss = 0.0\n",
        "        accuracy = 0.0\n",
        "\n",
        "    recall_1 = 0.0\n",
        "    if print_confusion and len(all_labels) > 0:\n",
        "        target_names = [\"not depressed\", \"depressed\"]\n",
        "        cr_str = classification_report(all_labels, all_preds, target_names=target_names, digits=6)\n",
        "        print(\"=== [Segment-level] Classification Report ===\")\n",
        "        print(cr_str)\n",
        "\n",
        "        cr_dict = classification_report(all_labels, all_preds, target_names=target_names,\n",
        "                                        digits=6, output_dict=True)\n",
        "        if \"depressed\" in cr_dict:\n",
        "            recall_1 = cr_dict[\"depressed\"][\"recall\"]\n",
        "\n",
        "        cm = confusion_matrix(all_labels, all_preds, labels=[0,1])\n",
        "        print(\"[Segment-level] Confusion Matrix ( [0,1] ):\\n\", cm)\n",
        "\n",
        "    return avg_loss, accuracy, recall_1\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 5-B) Participant-level Evaluation\n",
        "# -----------------------------------------------------\n",
        "def evaluate_one_epoch_participant_level(model, data_loader, criterion, device, print_confusion=True):\n",
        "    \"\"\"\n",
        "    Average the logits of all segments for each participant and make a final prediction based on the mean.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # While we could calculate segment-level loss here,\n",
        "    # this function primarily focuses on averaging loss across all segments.\n",
        "    total_loss = 0.0\n",
        "    total_segment_count = 0\n",
        "\n",
        "    # participant_id => [list of logits], {label}\n",
        "    participant_logits_map = defaultdict(list)\n",
        "    participant_label_map = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            sample_indices, wave_batch, sr_batch, lbl_batch, participant_ids = batch\n",
        "\n",
        "            wave_batch = wave_batch.to(device)\n",
        "            sr_batch   = sr_batch.to(device)\n",
        "            lbl_batch  = lbl_batch.to(device)\n",
        "\n",
        "            inputs = wave_to_wav2vec2_input(wave_batch, sr_batch, target_sr=16000).to(device)\n",
        "            logits = model(inputs)  # (B, num_classes)\n",
        "\n",
        "            # Segment-level loss (for reference)\n",
        "            loss_val = criterion(logits, lbl_batch.long())\n",
        "            bs = wave_batch.size(0)\n",
        "            total_loss += loss_val.item() * bs\n",
        "            total_segment_count += bs\n",
        "\n",
        "            # Store logits for each participant_id\n",
        "            for i in range(bs):\n",
        "                pid = participant_ids[i]\n",
        "                participant_logits_map[pid].append(logits[i].cpu().numpy())  # shape=(num_classes,)\n",
        "                # The label should be the same for all segments of a participant, so storing it once is sufficient (overwriting is fine).\n",
        "                participant_label_map[pid] = int(lbl_batch[i].item())\n",
        "\n",
        "    if total_segment_count > 0:\n",
        "        avg_loss = total_loss / total_segment_count\n",
        "    else:\n",
        "        avg_loss = 0.0\n",
        "\n",
        "    # Make final predictions at the participant level\n",
        "    all_participant_preds = []\n",
        "    all_participant_labels = []\n",
        "    for pid, list_of_logits in participant_logits_map.items():\n",
        "        # e.g., list_of_logits has shape [num_segments, 2]\n",
        "        avg_logit = np.mean(list_of_logits, axis=0)  # => (2,)\n",
        "        pred_label = np.argmax(avg_logit)\n",
        "        true_label = participant_label_map[pid]\n",
        "        all_participant_preds.append(pred_label)\n",
        "        all_participant_labels.append(true_label)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    participant_count = len(all_participant_labels)\n",
        "    if participant_count > 0:\n",
        "        correct_count = sum(\n",
        "            1 for p, t in zip(all_participant_preds, all_participant_labels) if p == t\n",
        "        )\n",
        "        participant_level_acc = correct_count / participant_count\n",
        "    else:\n",
        "        participant_level_acc = 0.0\n",
        "\n",
        "    recall_1 = 0.0\n",
        "    if print_confusion and participant_count > 0:\n",
        "        target_names = [\"not depressed\", \"depressed\"]\n",
        "        cr_str = classification_report(all_participant_labels, all_participant_preds,\n",
        "                                       target_names=target_names, digits=6)\n",
        "        print(\"=== [Participant-level] Classification Report ===\")\n",
        "        print(cr_str)\n",
        "\n",
        "        cr_dict = classification_report(all_participant_labels, all_participant_preds,\n",
        "                                        target_names=target_names,\n",
        "                                        digits=6, output_dict=True)\n",
        "        if \"depressed\" in cr_dict:\n",
        "            recall_1 = cr_dict[\"depressed\"][\"recall\"]\n",
        "\n",
        "        cm = confusion_matrix(all_participant_labels, all_participant_preds, labels=[0,1])\n",
        "        print(\"[Participant-level] Confusion Matrix ( [0,1] ):\\n\", cm)\n",
        "\n",
        "    return avg_loss, participant_level_acc, recall_1\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 6) Main\n",
        "# -----------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # === Settings ===\n",
        "    CSV_PATH = \"/content/drive/MyDrive/DAIC-WOZ/dataset_info_all_text.csv\"\n",
        "\n",
        "    BATCH_SIZE = 2\n",
        "    accumulation_steps = 16\n",
        "\n",
        "    # Number of epochs\n",
        "    EPOCHS = 15\n",
        "\n",
        "    LEARNING_RATE = 1e-5\n",
        "\n",
        "    # Class weights (example: [1.0, 2.5] for \"not depressed\", \"depressed\")\n",
        "    class_weights = torch.FloatTensor([1.0, 2.5])\n",
        "\n",
        "    # Load CSV\n",
        "    df_all = pd.read_csv(CSV_PATH)\n",
        "    print(f\"Loaded CSV total rows: {len(df_all)}\")\n",
        "\n",
        "    # ===================================================\n",
        "    # Split into train/dev/test by participant_id\n",
        "    #   - 60% train, 20% dev, 20% test\n",
        "    # ===================================================\n",
        "    unique_ids = df_all[\"participant_id\"].unique()\n",
        "\n",
        "    # Split1: train (60%) vs. remain (40%)\n",
        "    train_ids, remain_ids = train_test_split(\n",
        "        unique_ids, test_size=0.40, random_state=SEED, shuffle=True\n",
        "    )\n",
        "    # Split2: dev (20%) vs. test (20%) => half each of the remaining 40%\n",
        "    dev_ids, test_ids = train_test_split(\n",
        "        remain_ids, test_size=0.5, random_state=SEED, shuffle=True\n",
        "    )\n",
        "\n",
        "    print(f\"Unique IDs total: {len(unique_ids)}\")\n",
        "    print(f\"Train IDs: {len(train_ids)}, Dev IDs: {len(dev_ids)}, Test IDs: {len(test_ids)}\")\n",
        "\n",
        "    # Filter rows\n",
        "    train_df_raw = df_all[df_all[\"participant_id\"].isin(train_ids)].copy()\n",
        "    dev_df_raw   = df_all[df_all[\"participant_id\"].isin(dev_ids)].copy()\n",
        "    test_df_raw  = df_all[df_all[\"participant_id\"].isin(test_ids)].copy()\n",
        "\n",
        "    print(f\"Train data rows: {len(train_df_raw)}, Dev rows: {len(dev_df_raw)}, Test rows: {len(test_df_raw)}\")\n",
        "\n",
        "    # Create Dataset\n",
        "    train_dataset = AudioDataset(train_df_raw, target_sr=16000, verbose=False)\n",
        "    dev_dataset   = AudioDataset(dev_df_raw,   target_sr=16000, verbose=False)\n",
        "    test_dataset  = AudioDataset(test_df_raw,  target_sr=16000, verbose=False)\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn_audio)\n",
        "    dev_loader   = DataLoader(dev_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_audio)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn_audio)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"device={device}\")\n",
        "\n",
        "    # Model\n",
        "    model = AudioOnlyModel(\n",
        "        audio_model_name=\"facebook/wav2vec2-large-960h\",\n",
        "        freeze_feature_extractor=True,\n",
        "        cross_hidden_dim=384,\n",
        "        num_classes=2\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    # For logging metrics and saving\n",
        "    epoch_stats = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        # -------------------- Training --------------------\n",
        "        model.train()\n",
        "        total_loss_train = 0.0\n",
        "        total_samples_train = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            sample_indices, wave_batch, sr_batch, lbl_batch, participant_ids = batch\n",
        "\n",
        "            wave_batch = wave_batch.to(device)\n",
        "            sr_batch   = sr_batch.to(device)\n",
        "            lbl_batch  = lbl_batch.to(device)\n",
        "\n",
        "            inputs = wave_to_wav2vec2_input(wave_batch, sr_batch, 16000).to(device)\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, lbl_batch.long())\n",
        "\n",
        "            bs = wave_batch.size(0)\n",
        "            total_loss_train += loss.item() * bs\n",
        "            total_samples_train += bs\n",
        "\n",
        "            # Gradient accumulation\n",
        "            loss = loss / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (step + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            # Simple log every 50 steps\n",
        "            if (global_step % 50) == 0:\n",
        "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "                print(f\"[Train] epoch={epoch+1}, global_step={global_step}, loss={loss.item():.4f}, lr={current_lr}\")\n",
        "\n",
        "        # Average train loss\n",
        "        if total_samples_train > 0:\n",
        "            epoch_train_loss = total_loss_train / total_samples_train\n",
        "        else:\n",
        "            epoch_train_loss = 0.0\n",
        "\n",
        "        print(f\"[Epoch {epoch+1}] train loss = {epoch_train_loss:.4f}\")\n",
        "\n",
        "        # -------------------- Dev Evaluation (Segment-level) --------------------\n",
        "        print(f\"\\n=== [Epoch {epoch+1}] Dev Evaluation (Segment-level) ===\")\n",
        "        dev_loss_seg, dev_acc_seg, dev_recall_seg = evaluate_one_epoch_segment_level(\n",
        "            model, dev_loader, criterion, device, print_confusion=False\n",
        "        )\n",
        "        print(f\"Dev (segment-level): loss={dev_loss_seg:.4f}, acc={dev_acc_seg:.4f}, recall(depressed)={dev_recall_seg:.4f}\")\n",
        "\n",
        "        # -------------------- Dev Evaluation (Participant-level) --------------------\n",
        "        print(f\"\\n=== [Epoch {epoch+1}] Dev Evaluation (Participant-level) ===\")\n",
        "        dev_loss_part, dev_acc_part, dev_recall_part = evaluate_one_epoch_participant_level(\n",
        "            model, dev_loader, criterion, device, print_confusion=False\n",
        "        )\n",
        "        print(f\"Dev (participant-level): loss={dev_loss_part:.4f}, acc={dev_acc_part:.4f}, recall(depressed)={dev_recall_part:.4f}\")\n",
        "\n",
        "        # -------------------- Test Evaluation (Segment-level) -------------------\n",
        "        print(f\"\\n=== [Epoch {epoch+1}] Test Evaluation (Segment-level) ===\")\n",
        "        test_loss_seg, test_acc_seg, test_recall_seg = evaluate_one_epoch_segment_level(\n",
        "            model, test_loader, criterion, device, print_confusion=True\n",
        "        )\n",
        "        print(f\"Test (segment-level): loss={test_loss_seg:.4f}, acc={test_acc_seg:.4f}, recall(depressed)={test_recall_seg:.4f}\")\n",
        "\n",
        "        # -------------------- Test Evaluation (Participant-level) -------------------\n",
        "        print(f\"\\n=== [Epoch {epoch+1}] Test Evaluation (Participant-level) ===\")\n",
        "        test_loss_part, test_acc_part, test_recall_part = evaluate_one_epoch_participant_level(\n",
        "            model, test_loader, criterion, device, print_confusion=True\n",
        "        )\n",
        "        print(f\"Test (participant-level): loss={test_loss_part:.4f}, acc={test_acc_part:.4f}, recall(depressed)={test_recall_part:.4f}\")\n",
        "\n",
        "        # -------------------- Save metrics in memory -------------------\n",
        "        epoch_stats.append({\n",
        "            \"epoch\": epoch+1,\n",
        "            \"train_loss\": epoch_train_loss,\n",
        "            \"dev_loss_segment_level\": dev_loss_seg,\n",
        "            \"dev_acc_segment_level\": dev_acc_seg,\n",
        "            \"dev_recall_depressed_segment_level\": dev_recall_seg,\n",
        "            \"dev_loss_participant_level\": dev_loss_part,\n",
        "            \"dev_acc_participant_level\": dev_acc_part,\n",
        "            \"dev_recall_depressed_participant_level\": dev_recall_part,\n",
        "            \"test_loss_segment_level\": test_loss_seg,\n",
        "            \"test_acc_segment_level\": test_acc_seg,\n",
        "            \"test_recall_depressed_segment_level\": test_recall_seg,\n",
        "            \"test_loss_participant_level\": test_loss_part,\n",
        "            \"test_acc_participant_level\": test_acc_part,\n",
        "            \"test_recall_depressed_participant_level\": test_recall_part\n",
        "        })\n",
        "\n",
        "        # Optional: Save CSV report periodically (e.g., every 5 epochs)\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            df_report = pd.DataFrame(epoch_stats)\n",
        "            save_path = f\"/content/drive/MyDrive/wav2vec_epoch_report_up_to_{epoch+1}.csv\"\n",
        "            df_report.to_csv(save_path, index=False)\n",
        "            print(f\"[Info] Saved epoch report at: {save_path}\")\n",
        "\n",
        "        print(f\"--- End of epoch {epoch+1} ---\\n\")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "    # Optionally, save final report as well\n",
        "    df_final = pd.DataFrame(epoch_stats)\n",
        "    df_final.to_csv(\"/content/drive/MyDrive/wav2vec_final_epoch_report.csv\", index=False)\n",
        "    print(\"[Info] Saved final report at /content/drive/MyDrive/wav2vec_final_epoch_report.csv\")"
      ],
      "metadata": {
        "id": "YpiWeWTQkd0V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}